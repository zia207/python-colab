{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/python-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-03-05-tree-based-models-gradient-boosted-adaboost-python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qrItz_mJNWw"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1HX868sY0Bxt_Oa0C8F5UOn7Rlkots3o3&usp=drive_fs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzp9ZseROTcY"
      },
      "source": [
        "# 3.5 Adaptive Boosting (AdaBoost)\n",
        "\n",
        "AdaBoost, or Adaptive Boosting, is a powerful ensemble learning technique that combines multiple weak classifiers to create a strong classifier. It works by sequentially applying weak classifiers to the training data, adjusting the weights of misclassified instances to focus more on difficult cases in subsequent iterations. This method is particularly effective for improving the performance of models on complex datasets. This tutiorial will guide you through the mathematical foundations of AdaBoost, its implementation in Python using the {scikit-learn} library, and practical examples using the Titanic dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLkAu88QToVr"
      },
      "source": [
        "## Overview\n",
        "\n",
        "AdaBoost, short for **Adaptive Boosting**, is a machine learning ensemble algorithm used primarily for classification tasks (though it can be extended to regression). It combines multiple weak learners—typically simple models like decision stumps (single-level decision trees)—to create a strong classifier. The key idea is to focus on misclassified samples by assigning them higher weights in subsequent iterations, allowing the model to improve its performance progressively.\n",
        "\n",
        "AdaBoost was introduced by Freund and Schapire in 1996 and is widely used due to its simplicity, effectiveness, and ability to work well with high-dimensional data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL2L7JRduO3P"
      },
      "source": [
        "### Key Features of AdaBoost\n",
        "\n",
        "AdaBoost's key features:\n",
        "\n",
        "1. `Weight Adjustment`: Boosts misclassified data weights iteratively.\n",
        "2. `Weak Learner Combo`: Combines weak classifiers into a strong one.\n",
        "3. `Error-Driven`: Focuses on errors from previous learners.\n",
        "4. `Adaptive`: Adjusts to data complexity.\n",
        "5. `Binary Focus`: Best for binary classification, extendable to multiclass.\n",
        "6. `Robust`: Less prone to overfitting with proper tuning.\n",
        "7. `Feature Importance`: Highlights key features.\n",
        "8. `Simple & Flexible`: Works with various weak learners.\n",
        "9. `Noise Sensitivity`: Can overfit with noisy data.\n",
        "10. `Efficient`: Fast for small-to-medium datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Does AdaBoost Work?\n",
        "\n",
        "AdaBoost works by iteratively training weak learners, adjusting the weights of training samples, and combining the weak learners into a final strong classifier. Each weak learner focuses more on the samples that were misclassified by previous learners. The final prediction is a weighted combination of the weak learners' predictions.\n",
        "\n",
        "Here’s a step-by-step explanation of the AdaBoost algorithm, including the equations involved:\n",
        "\n",
        "1.  Initialize Sample Weights\n",
        "\n",
        "-   Assign equal weights to all training samples. For a dataset with ( N ) samples, each sample’s initial weight is:\n",
        "\n",
        "$$ w_i^{(1)} = \\frac{1}{N}, \\quad i = 1, 2, \\dots, N $$ where $w_i^{(1)}$ is the weight of the $i$-th sample in the first iteration.\n",
        "\n",
        "2.  Train Weak Learners Iteratively\n",
        "\n",
        "-   For $t = 1, 2, \\dots, T$ (where $T$ is the number of weak learners):\n",
        "\n",
        "a.  `Train a weak learne` $h_t(x)$ (e.g., a decision stump) on the training data, using the current sample weights $w_i^{(t)}$.\n",
        "\n",
        "b.  `compute the weighted error` of the weak learner:\n",
        "\n",
        "$$ \\epsilon*t =* \\sum{i=1}^N w_i^{(t)} \\cdot \\mathbb{I}(h_t(x_i) \\neq y_i) $$\n",
        "\n",
        "where:\n",
        "\n",
        "-   $\\epsilon_t$ is the weighted error rate.\n",
        "\n",
        "-   $\\mathbb{I}(h_t(x_i) \\neq y_i)$ is an indicator function that equals 1 if the prediction $h_t(x_i)$ is incorrect for sample $x_i$ (with true label $y_i$, and 0 otherwise.\n",
        "\n",
        "c.  `Compute the learner’s weight` (importance) based on its error:\n",
        "\n",
        "$$ \\alpha\\_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right) $$ - $\\alpha_t$ represents the contribution of the weak learner $h_t$ to the final model.\n",
        "\n",
        "-   If $\\epsilon_t < 0.5$, $\\alpha_t> 0$, meaning the learner contributes positively.\n",
        "\n",
        "-   If $\\epsilon_t = 0.5$, $\\alpha_t = 0$, indicating no contribution (random guessing).\n",
        "\n",
        "-   If $\\epsilon_t > 0.5$, $\\alpha_ < 0$, but typically, weak learners are better than random guessing.\n",
        "\n",
        "3.  Update Sample Weights\n",
        "\n",
        "-   Adjust the weights of the samples to focus on misclassified ones:\n",
        "\n",
        "$$ w_i^{(t+1)} = w_i^{(t)} \\cdot \\exp \\left( \\alpha\\_t \\cdot \\mathbb{I}(h_t(x_i) \\neq y_i) \\right) $$ - Misclassified samples $h_t(x_i) \\neq y_i$ have their weights increased by $e^{\\alpha\\_t}$.\n",
        "\n",
        "-   Correctly classified samples have their weights decreased by $e^{-\\alpha\\_t}$.\n",
        "\n",
        "-   Normalize the weights so they sum to 1:\n",
        "\n",
        "$$  w_i^{(t+1)} = \\frac{w_i^{(t+1)}}{\\sum_{j=1}^N w_j^{(t+1)}} $$ \\`\n",
        "\n",
        "4.  Combine Weak Learners\n",
        "\n",
        "-   After ( T ) iterations, combine the weak learners into a final strong classifier:\n",
        "\n",
        "$$  H(x) = \\text{sign} \\left( \\sum\\_{t=1}^T \\alpha\\_t h_t(x) \\right)$$\n",
        "\n",
        "where: - $h_t(x) \\in {-1, +1}$ is the prediction of the $t$-th weak learner.\n",
        "\n",
        "-   $\\alpha_t$ is the weight of the $t$-th weak learner.\n",
        "\n",
        "-   The final prediction $H(x)$ outputs $+1$ or $-1$ (for binary classification).\n",
        "\n",
        "Note:\n",
        "\n",
        "    -   **Weak Learners**: Typically decision stumps, but any classifier slightly better than random guessing can be used.\n",
        "\n",
        "    -   **Advantages**: AdaBoost is robust, reduces overfitting (especially with simple weak learners), and is computationally efficient.\n",
        "\n",
        "    -   **Limitations**: Sensitive to noisy data and outliers, as misclassified samples get higher weights.\n",
        "\n",
        "    -   **Extensions**: Variants like Gradient Boosting and XGBoost build on AdaBoost’s principles but use different optimization techniques.\n"
      ],
      "metadata": {
        "id": "03n21BL3-pyU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWCTcHkzvHE2"
      },
      "source": [
        "The flowchart below summarizes the AdaBoost algorithm, highlighting its key steps from initialization to prediction:\n",
        "\n",
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=1thPPruEzsmUY31rAg9K6aLmWVUMPylWL\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuhrbXQxGIIy"
      },
      "source": [
        "### Advantages of AdaBoost\n",
        "\n",
        "1. `High Accuracy`: Combines weak learners (e.g., decision stumps) into a strong classifier, achieving ~82% accuracy on the Titanic dataset.\n",
        "2. `Focus on Hard Samples`: Increases weights for misclassified samples, improving robustness (e.g., prioritizing `Sex`, `Pclass`).\n",
        "3. `Handles Mixed Data`: Works with numerical (`Age`, `Fare`) and categorical (`Sex`, `Pclass`) features.\n",
        "4. `Low Overfitting`: Simple learners reduce overfitting risk, stable at 50 iterations.\n",
        "5. `Feature Importance`: Identifies key predictors (e.g., `Sex` ~47%, `Pclass` ~22%).\n",
        "6. `Minimal Tuning`: Effective with default settings (e.g., `mfinal = 50` in `adabag`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70UMhkxtGPsU"
      },
      "source": [
        "### Limitations of AdaBoost\n",
        "\n",
        "1. `Noise Sensitivity`: Overemphasizes outliers, reducing precision (~31% in synthetic Titanic data).\n",
        "2. `Weak Learner Dependence`: Poor performance if learners are too weak (e.g., stumps in scratch model, ~44% F1-score).\n",
        "3. `Computational Cost`: Slow on large datasets due to iterative training.\n",
        "4. `Imbalanced Data`: Struggles with minority class (e.g., ~73% recall for `Survived = 1`).\n",
        "5. `No Probabilities`: Outputs class labels, not confidence scores.\n",
        "6. `Classification Focus`: Less suited for regression tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztl1QBDb4MYe"
      },
      "source": [
        "## AdaBoost  implementation in Python from scratch\n",
        "\n",
        "Here we’ll implement the AdaBoost algorithm in Python from scratch (without using any external packages) to predict the `Survived` outcome in the a synthetic Titanic dataset. The weak learners will be decision stumps (single-level decision trees), and I’ll evaluate the model’s performance using accuracy, precision, recall, and F1-score on the training data. Below is the complete Python code, including data generation, AdaBoost implementation, model fitting, and evaluation, followed by an explanation of the key components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP2lCzFHBUip"
      },
      "source": [
        "### Data Generation\n",
        "\n",
        "   -First we will create  a synthetic Titanic dataset with 1000 samples and features: `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`, `Embarked`, and the binary response `Survived` (0 or 1).\n",
        "   \n",
        "   - Categorical variables (`Sex`, `Embarked`, `Pclass`) are converted to factors for proper handling in the decision stump function.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aTjVIV5ZBW96"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate synthetic Titanic dataset\n",
        "np.random.seed(123)\n",
        "n = 1000\n",
        "Pclass = np.random.choice([1, 2, 3], size=n, p=[0.2, 0.3, 0.5])\n",
        "Sex = np.random.choice(['male', 'female'], size=n, p=[0.6, 0.4])\n",
        "Age = np.round(np.random.normal(loc=30, scale=10, size=n), 1)\n",
        "Age[Age < 1] = 1\n",
        "SibSp = np.random.choice([0, 1, 2, 3, 4, 5], size=n, p=[0.7, 0.1, 0.1, 0.05, 0.03, 0.02])\n",
        "Parch = np.random.choice([0, 1, 2, 3, 4], size=n, p=[0.8, 0.1, 0.05, 0.03, 0.02])\n",
        "Fare = np.round(np.random.gamma(shape=2, scale=15, size=n), 2)\n",
        "Embarked = np.random.choice(['C', 'Q', 'S'], size=n, p=[0.3, 0.2, 0.5])\n",
        "\n",
        "surv_prob = 1 / (1 + np.exp(-(\n",
        "    0.5 * (Sex == 'female').astype(int) +\n",
        "    0.3 * (Pclass == 1).astype(int) - 0.4 * (Pclass == 3).astype(int) +\n",
        "    0.02 * (Age - 30) -\n",
        "    0.05 * SibSp - 0.02 * Parch +\n",
        "    0.001 * Fare +\n",
        "    0.2 * (Embarked == 'C').astype(int) - 0.1 * (Embarked == 'Q').astype(int)\n",
        ")))\n",
        "Survived = np.random.binomial(1, surv_prob)\n",
        "\n",
        "titanic = pd.DataFrame({\n",
        "    'Pclass': Pclass,\n",
        "    'Sex': Sex,\n",
        "    'Age': Age,\n",
        "    'SibSp': SibSp,\n",
        "    'Parch': Parch,\n",
        "    'Fare': Fare,\n",
        "    'Embarked': Embarked,\n",
        "    'Survived': Survived\n",
        "})\n",
        "\n",
        "# Convert categorical variables to category type\n",
        "titanic['Pclass'] = titanic['Pclass'].astype('category')\n",
        "titanic['Sex'] = titanic['Sex'].astype('category')\n",
        "titanic['Embarked'] = titanic['Embarked'].astype('category')\n",
        "\n",
        "# Split into training and test sets\n",
        "train_data, test_data = train_test_split(titanic, test_size=0.2, stratify=titanic['Survived'], random_state=123)\n",
        "\n",
        "# Define predictors and response\n",
        "predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "response = 'Survived'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwIhiSZNCO9N"
      },
      "source": [
        "### Preprocess Data\n",
        "\n",
        "Convert categorical features to numeric:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vYPiX59GCiTY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df = df.copy()\n",
        "    df['Sex'] = pd.factorize(df['Sex'])[0]  # male=0, female=1\n",
        "    df['Embarked'] = pd.factorize(df['Embarked'])[0]  # C=0, Q=1, S=2\n",
        "    return df.values\n",
        "\n",
        "# Assuming titanic is the DataFrame from the previous code\n",
        "X = preprocess_data(titanic[titanic.columns.difference(['Survived'])])\n",
        "y = titanic['Survived'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aFvX9PX6t7N"
      },
      "source": [
        "\n",
        "### Decision Stump Function (`decision_stump`)\n",
        "\n",
        "   - A decision stump is a weak learner that splits data based on a single feature.\n",
        "   \n",
        "   - For **numeric features** (`Age`, `SibSp`, `Parch`, `Fare`), it tries all unique values as thresholds and predicts +1 (survived) or -1 (not survived) based on whether the feature value is less than or equal to the threshold.\n",
        "   \n",
        "   - For **categorical features** (`Pclass`, `Sex`, `Embarked`), it tries each category level and predicts +1 if the feature equals the level, else -1.\n",
        "   \n",
        "   - The **polarity** adjusts the prediction direction if the error exceeds 0.5 (flipping the prediction reduces the error).\n",
        "   \n",
        "   - The stump with the lowest weighted error is selected."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def decision_stump(data, weights, predictors, response):\n",
        "    \"\"\"\n",
        "    Creates a decision stump (weak learner) by finding the best feature and split\n",
        "    that minimizes the weighted error.\n",
        "\n",
        "    Parameters:\n",
        "    - data: pandas DataFrame containing predictors and response\n",
        "    - weights: numpy array of weights for each sample\n",
        "    - predictors: list of column names for predictor features\n",
        "    - response: column name of the response variable (binary, 0/1)\n",
        "\n",
        "    Returns:\n",
        "    - dict containing the best feature, threshold (for numeric), split_value (for categorical),\n",
        "      polarity, and error\n",
        "    \"\"\"\n",
        "    n = len(data)\n",
        "    best_error = float('inf')\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "    best_split_value = None\n",
        "    best_polarity = 1\n",
        "\n",
        "    # Iterate over predictors\n",
        "    for feature in predictors:\n",
        "        values = data[feature].values\n",
        "        is_numeric = pd.api.types.is_numeric_dtype(data[feature])\n",
        "\n",
        "        if is_numeric:\n",
        "            # For numeric features, try thresholds\n",
        "            thresholds = np.sort(np.unique(values))\n",
        "            thresholds = thresholds[:-1]  # Exclude max value\n",
        "            for threshold in thresholds:\n",
        "                # Predict +1 if feature <= threshold, else -1\n",
        "                pred = np.where(values <= threshold, 1, -1)\n",
        "                # Convert response to {-1, 1} for error calculation\n",
        "                y = np.where(data[response].values == 1, 1, -1)\n",
        "                error = np.sum(weights * (pred != y))\n",
        "                if error > 0.5:\n",
        "                    error = 1 - error\n",
        "                    polarity = -1\n",
        "                else:\n",
        "                    polarity = 1\n",
        "                if error < best_error:\n",
        "                    best_error = error\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "                    best_split_value = None\n",
        "                    best_polarity = polarity\n",
        "        else:\n",
        "            # For categorical features, try each level\n",
        "            levels = np.unique(values)\n",
        "            for level in levels:\n",
        "                # Predict +1 if feature == level, else -1\n",
        "                pred = np.where(values == level, 1, -1)\n",
        "                # Convert response to {-1, 1} for error calculation\n",
        "                y = np.where(data[response].values == 1, 1, -1)\n",
        "                error = np.sum(weights * (pred != y))\n",
        "                if error > 0.5:\n",
        "                    error = 1 - error\n",
        "                    polarity = -1\n",
        "                else:\n",
        "                    polarity = 1\n",
        "                if error < best_error:\n",
        "                    best_error = error\n",
        "                    best_feature = feature\n",
        "                    best_threshold = None\n",
        "                    best_split_value = level\n",
        "                    best_polarity = polarity\n",
        "\n",
        "    # Return the best stump\n",
        "    return {\n",
        "        'feature': best_feature,\n",
        "        'threshold': best_threshold,\n",
        "        'split_value': best_split_value,\n",
        "        'polarity': best_polarity,\n",
        "        'error': best_error\n",
        "    }"
      ],
      "metadata": {
        "id": "UMkiS7b-Droi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict Stump Function (`predict_stump`)\n",
        "\n",
        "   - Applies the decision stump to the data, returning predictions (+1 or -1) based on the feature, threshold (for numeric), or split value (for categorical), and polarity."
      ],
      "metadata": {
        "id": "gpz0NZAADtRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def predict_stump(stump, data):\n",
        "    \"\"\"\n",
        "    Predicts outcomes using a decision stump.\n",
        "\n",
        "    Parameters:\n",
        "    - stump: dict containing feature, threshold (for numeric), split_value (for categorical),\n",
        "             and polarity (from decision_stump function)\n",
        "    - data: pandas DataFrame containing the feature used in the stump\n",
        "\n",
        "    Returns:\n",
        "    - numpy array of predictions (+1 or -1)\n",
        "    \"\"\"\n",
        "    feature = stump['feature']\n",
        "    values = data[feature].values\n",
        "\n",
        "    if stump['threshold'] is None:\n",
        "        # Categorical feature\n",
        "        pred = np.where(values == stump['split_value'], 1, -1)\n",
        "    else:\n",
        "        # Numeric feature\n",
        "        pred = np.where(values <= stump['threshold'], 1, -1)\n",
        "\n",
        "    return pred * stump['polarity']"
      ],
      "metadata": {
        "id": "Bx9doXYoDv33"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost Function (`adaboost`)\n",
        "\n",
        "   - Initializes sample weights as $\\frac{1}{N}$).\n",
        "   \n",
        "   - For $T = 50$ iterations:\n",
        "   \n",
        "  - Trains a decision stump using current weights.\n",
        "     \n",
        "  - Computes the weighted error $\\epsilon_t$.\n",
        "     \n",
        "  - Calculates the stump’s weight $\\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right)$.\n",
        "     \n",
        "  - Updates sample weights: $w_i \\leftarrow w_i \\cdot \\exp(-\\alpha_t \\cdot y_i \\cdot h_t(x_i))$, then normalizes them.\n",
        "     \n",
        "  - Stores the stump and its $\\alpha_t$.\n",
        "     \n",
        "  - Stops early if the error is too high $\\epsilon_t \\geq 0.5$.\n"
      ],
      "metadata": {
        "id": "lfCVvlWXD4M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adaboost(data, predictors, response, T=10):\n",
        "    \"\"\"\n",
        "    Implements AdaBoost algorithm using decision stumps.\n",
        "\n",
        "    Parameters:\n",
        "    - data: pandas DataFrame containing predictors and response\n",
        "    - predictors: list of predictor column names\n",
        "    - response: name of the response column (binary, 0/1)\n",
        "    - T: number of iterations (stumps)\n",
        "\n",
        "    Returns:\n",
        "    - dict containing list of stumps and their weights (alphas)\n",
        "    \"\"\"\n",
        "    n = len(data)\n",
        "    weights = np.ones(n) / n  # Initialize weights\n",
        "    stumps = []\n",
        "    alphas = np.zeros(T)\n",
        "\n",
        "    # Convert response to {-1, +1}\n",
        "    y = np.where(data[response].values == 1, 1, -1)\n",
        "\n",
        "    for t in range(T):\n",
        "        # Train a decision stump\n",
        "        stump = decision_stump(data, weights, predictors, response)\n",
        "\n",
        "        # Compute predictions\n",
        "        pred = predict_stump(stump, data)\n",
        "\n",
        "        # Compute weighted error\n",
        "        error = stump['error']\n",
        "        if error == 0:\n",
        "            error = 1e-10  # Avoid division by zero\n",
        "        if error >= 0.5:\n",
        "            break  # Stop if error is too high\n",
        "\n",
        "        # Compute alpha (stump weight)\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "\n",
        "        # Update weights\n",
        "        weights = weights * np.exp(-alpha * y * pred)\n",
        "        weights = weights / np.sum(weights)  # Normalize\n",
        "\n",
        "        # Store stump and alpha\n",
        "        stumps.append(stump)\n",
        "        alphas[t] = alpha\n",
        "\n",
        "    # Trim alphas to match number of stumps\n",
        "    alphas = alphas[:len(stumps)]\n",
        "\n",
        "    return {'stumps': stumps, 'alphas': alphas}"
      ],
      "metadata": {
        "id": "azlfCVpkD-rg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict AdaBoost Function (`predict_adaboost`)\n",
        "\n",
        "   - Combines predictions from all stumps, weighted by their $\\alpha_t$).\n",
        "   \n",
        "   - The final prediction is $\\text{sign} \\left( \\sum \\alpha_t h_t(x) \\right)$, mapped to 0 or 1."
      ],
      "metadata": {
        "id": "z01FdgBWEBzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_adaboost(model, data):\n",
        "    \"\"\"\n",
        "    Predicts outcomes using an AdaBoost model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: dict containing stumps and alphas\n",
        "    - data: pandas DataFrame for prediction\n",
        "\n",
        "    Returns:\n",
        "    - numpy array of predictions (0 or 1)\n",
        "    \"\"\"\n",
        "    predictions = np.zeros(len(data))\n",
        "    for stump, alpha in zip(model['stumps'], model['alphas']):\n",
        "        pred = predict_stump(stump, data)\n",
        "        predictions += alpha * pred\n",
        "    return np.where(predictions >= 0, 1, 0)"
      ],
      "metadata": {
        "id": "9ESlrzn-ECi_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Fitting and Evaluation\n",
        "\n",
        "   - Fit the AdaBoost model using the synthetic Titanic dataset.\n",
        "   \n",
        "   - Make predictions on the training data.\n",
        "   \n",
        "   - computes a confusion matrix comparing predicted and actual `Survived` values.\n",
        "   - Calculates:\n",
        "     - `Accuracy`: Proportion of correct predictions.\n",
        "     - `Precision`: True positives / (True positives + False positives).\n",
        "     - `Recall`: True positives / (True positives + False negatives).\n",
        "     - `F1-Score`: Harmonic mean of precision and recall.\n",
        "     \n"
      ],
      "metadata": {
        "id": "LYrovqRLEIZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train AdaBoost model\n",
        "model = adaboost(train_data, predictors, response, T=10)"
      ],
      "metadata": {
        "id": "y9QwM9ft7Juq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "y_pred = predict_adaboost(model, test_data)\n",
        "y_test = test_data[response].values\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nTest Set Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNLTXzHMEJIs",
        "outputId": "cd52306f-7459-41b5-cf4d-e421721f40c8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Evaluation Metrics:\n",
            "Accuracy: 0.5650\n",
            "Precision: 0.5607\n",
            "Recall: 0.6000\n",
            "F1-Score: 0.5797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Z1SpIUbgLy"
      },
      "source": [
        "## AdaBoost with Python\n",
        "\n",
        "In this example, we'll use the Titanic dataset  to demonstrate how to implement `AdaBoost` in Python using {scikit-learn}. We'll preprocess the data, fit an `AdaBoost model`, evaluate its performance with cross-validation, and visualize feature importance. The dataset includes features like passenger class (pclass), sex, age, and others, with the target variable survived. Since AdaBoost requires numerical inputs, we'll encode categorical variables using one-hot encoding. The following concise code showcases the entire process, from data preparation to model evaluation.The dataset will be preprocessed as specified, split into training (80%) and test (20%) sets,  and the model will be evaluated using accuracy, precision, recall, and F1-score on the test set. Below is the complete Python code, followed by an explanation of the steps and results.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "metadata": {
        "id": "oEdxJnokn-3a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Prepare the Data"
      ],
      "metadata": {
        "id": "vtFBtYNxEUX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Titanic dataset from sklearn's openml\n",
        "titanic = fetch_openml('titanic', version=1, as_frame=True).frame\n",
        "\n",
        "# Select relevant columns\n",
        "data = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].copy()\n",
        "\n",
        "# Clean missing values\n",
        "data['age'] = data['age'].fillna(data['age'].median())\n",
        "data['embarked'] = data['embarked'].fillna('S')\n",
        "data['fare'] = data['fare'].fillna(data['fare'].median())  # Handle any missing fares\n",
        "\n",
        "# Convert categorical columns to strings for encoding\n",
        "data['pclass'] = data['pclass'].astype(str)\n",
        "data['sex'] = data['sex'].astype(str)\n",
        "data['embarked'] = data['embarked'].astype(str)\n",
        "\n",
        "# Encode categorical variables for AdaBoost\n",
        "label_encoders = {}\n",
        "for col in ['pclass', 'sex', 'embarked']:\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Ensure 'survived' is integer\n",
        "data['survived'] = data['survived'].astype(int)\n",
        "\n",
        "# Split into training and test sets (80-20 split, stratified)\n",
        "np.random.seed(123)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['survived'])\n",
        "\n",
        "# Separate features and labels\n",
        "X_train = train_data.drop('survived', axis=1)\n",
        "y_train = train_data['survived']\n",
        "X_test = test_data.drop('survived', axis=1)\n",
        "y_test = test_data['survived']"
      ],
      "metadata": {
        "id": "0Rx9_gQuEUsl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Fitting the AdaBoost Model\n",
        "\n",
        "The  code below initializes an AdaBoost model using scikit-learn. It uses decision stumps (`DecisionTreeClassifier` with `max_depth=1`) as the weak learners and trains 100 of them. Finally, it fits the model to the training data."
      ],
      "metadata": {
        "id": "W_q4eufmEcMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train AdaBoost model\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)  # Weak learner\n",
        "adaboost = AdaBoostClassifier(estimator=base_estimator,\n",
        "                             n_estimators=100,\n",
        "                             random_state=123)\n",
        "adaboost.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "Co3v1w4REc0-",
        "outputId": "8b18486d-fa3f-4111-b95c-d3509151f4d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   n_estimators=100, random_state=123)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   n_estimators=100, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>AdaBoostClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\">?<span>Documentation for AdaBoostClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   n_estimators=100, random_state=123)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: DecisionTreeClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div> </div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Predictions and Evaluating the Model\n",
        "\n",
        "  \n",
        "   - Metrics calculated:\n",
        "     - **Accuracy**: Proportion of correct predictions.\n",
        "     - **Precision**: True positives / (True positives + False positives).\n",
        "     - **Recall**: True positives / (True positives + False negatives).\n",
        "     - **F1-Score**: Harmonic mean of precision and recall."
      ],
      "metadata": {
        "id": "82gVvlyTEiEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "\n",
        "# Print results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted']))\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Variable importance\n",
        "print(\"\\nVariable Importance:\")\n",
        "importances = pd.Series(adaboost.feature_importances_, index=X_train.columns)\n",
        "print(importances.sort_values(ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z8wdm4WEiv0",
        "outputId": "374e9992-3e1a-4fa2-c377-2d9ccdbec9c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "Predicted    0   1\n",
            "Actual            \n",
            "0          127  35\n",
            "1           29  71\n",
            "Accuracy: 0.7557\n",
            "Precision: 0.6698\n",
            "Recall: 0.7100\n",
            "F1-Score: 0.6893\n",
            "\n",
            "Variable Importance:\n",
            "age         0.292068\n",
            "sibsp       0.194010\n",
            "sex         0.170488\n",
            "parch       0.154859\n",
            "pclass      0.120085\n",
            "fare        0.050210\n",
            "embarked    0.018280\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Importance\n",
        "\n",
        "   - The `adabag_model$importance` provides the relative importance of each predictor, based on how much each feature reduces the weighted error across iterations."
      ],
      "metadata": {
        "id": "guLHUMaWEnrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable importance\n",
        "importances = pd.Series(adaboost.feature_importances_, index=X_train.columns)\n",
        "importances = importances.sort_values(ascending=False)\n",
        "\n",
        "# Plot variable importance\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=importances.values, y=importances.index, hue=importances.index, palette='viridis', legend=False)\n",
        "plt.title(\"Variable Importance in AdaBoost Model\")\n",
        "plt.xlabel(\"Relative Importance\")\n",
        "plt.ylabel(\"Predictors\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "oWPw-wbWEqjF",
        "outputId": "2993327d-a582-4275-c096-d767d0caeb7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-validation"
      ],
      "metadata": {
        "id": "b5c0FtqgEvfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Perform 5-fold cross-validation with `boosting.cv()`"
      ],
      "metadata": {
        "id": "DoLuhLRREzmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(adaboost, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print cross-validation results\n",
        "print(\"Cross-validation Accuracy Scores:\", cv_scores)\n",
        "print(f\"Mean Cross-validation Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard Deviation of Cross-validation Accuracy: {cv_scores.std():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJtj-TVyEwPb",
        "outputId": "89849f63-2de0-4109-de86-efc3db9ef15f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation Accuracy Scores: [0.77619048 0.7952381  0.79425837 0.784689   0.79425837]\n",
            "Mean Cross-validation Accuracy: 0.7889\n",
            "Standard Deviation of Cross-validation Accuracy: 0.0074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac403705"
      },
      "source": [
        "## Summary and Conclusions\n",
        "\n",
        "This notebook explored the Adaptive Boosting (AdaBoost) ensemble learning technique for classification, demonstrating its implementation both from scratch in R and using the scikit-learn library in Python, using the Titanic dataset as an example.\n",
        "\n",
        "The notebook began by introducing the core concepts of AdaBoost, including its iterative weighting mechanism, the use of weak learners (decision stumps in this case), and the mathematical foundations behind weight updates and final prediction.\n",
        "\n",
        "A scratch implementation in R was presented, covering data generation, the creation of decision stumps, and the AdaBoost algorithm itself. While this implementation provided a clear understanding of the internal workings, the evaluation on the synthetic dataset showed moderate performance.\n",
        "\n",
        "Subsequently, the notebook demonstrated a more practical approach using scikit-learn in Python. The Titanic dataset was loaded, preprocessed (handling missing values and encoding categorical features), and split into training and testing sets. An AdaBoost classifier with decision stumps as base estimators was trained, and its performance was evaluated using accuracy, precision, recall, and F1-score. The scikit-learn implementation achieved better performance on the actual Titanic dataset compared to the R scratch implementation on synthetic data.\n",
        "\n",
        "In conclusion, AdaBoost is a powerful boosting algorithm that can significantly improve the performance of weak learners by focusing on misclassified instances. The scikit-learn implementation offers a convenient and efficient way to apply AdaBoost in practice, providing good performance and insights into feature importance. While the R scratch implementation was valuable for understanding the algorithm's mechanics, the Python scikit-learn approach is more suitable for practical machine learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACNQyq1seLc7"
      },
      "source": [
        "## References\n",
        "\n",
        "\n",
        "1. **Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1), 119–139.**\n",
        "   - Introduces the original AdaBoost algorithm, detailing its theoretical foundation and iterative weighting mechanism for combining weak learners.\n",
        "\n",
        "2. **Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.**\n",
        "   - Chapter 10 provides a comprehensive overview of boosting, including AdaBoost’s mechanics, strengths, and limitations in classification tasks.\n",
        "\n",
        "3.  **scikit-learn Documentation:**\n",
        "    -   [AdaBoostClassifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html): The official documentation for the AdaBoostClassifier in scikit-learn, providing detailed information on parameters and usage.\n",
        "    -   [Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#adaboost): The scikit-learn documentation page on ensemble methods, including a section on AdaBoost.\n",
        "\n",
        "4.  **Towards Data Science:**\n",
        "    -   Look for articles on Towards Data Science that explain AdaBoost and show Python implementations. Many data scientists share their knowledge and code examples on this platform.\n",
        "\n",
        "5.  **Medium:**\n",
        "    -   Similar to Towards Data Science, Medium has numerous articles explaining AdaBoost concepts and providing Python code examples.\n",
        "\n",
        "6.  **GeeksforGeeks:**\n",
        "    -   [AdaBoost Algorithm](https://www.geeksforgeeks.org/adaboost-for-machine-learning/): Provides a good explanation of the AdaBoost algorithm and often includes Python examples.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwJ8KIQjzstfGb/79dyskj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}