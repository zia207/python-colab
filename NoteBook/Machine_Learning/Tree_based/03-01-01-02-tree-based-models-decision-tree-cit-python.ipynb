{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOuf+1UKl47rPzpUFDTOQ46",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/python-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-02-tree-based-models-decision-tree-cit-python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1xLlN9eEG2IYFBlAuwl53aDVxcBkRnkEw\n",
        ")"
      ],
      "metadata": {
        "id": "4qrItz_mJNWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Conditional Inference Trees\n",
        "\n",
        "Conditional Inference Trees (CITs) are a type of decision tree algorithm used for both classification and regression tasks. Unlike traditional decision trees (e.g., CART), CITs are designed to reduce bias in variable selection and overfitting by using a statistical framework based on conditional inference. They rely on hypothesis testing to select splits, ensuring that the splitting process is statistically sound. Below, I explain how CITs work for classification and regression problems, detailing each step with the necessary mathematical foundations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dzp9ZseROTcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "CITs operate by recursively partitioning the input space into regions based on feature values, using a statistical test to determine the best split at each node. The key idea is to select splits that maximize the association between the predictor variables and the response variable, while controlling for multiple testing and avoiding bias toward variables with many possible splits.\n"
      ],
      "metadata": {
        "id": "mLkAu88QToVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How it works\n",
        "\n",
        "1. Define the Hypothesis Testing Framework\n",
        "\n",
        "CITs use a statistical hypothesis testing framework to decide whether and how to split a node. The goal is to test whether a predictor variable $X_j$ is independent of the response variable $Y$, conditional on the current node’s data.\n",
        "\n",
        "-   `Null Hypothesis` ($H_0$): The predictor $X_j$ and the response $Y$ are independent, i.e., the distribution of $Y$ does not depend on $X_j$.\n",
        "\n",
        "-   `Alternative Hypothesis` ($H_1$): There is a dependence between $X_j$ and $Y$.\n",
        "\n",
        "The test statistic is based on the association between $X_j$ and $Y$. The choice of test statistic depends on the type of response variable:\n",
        "\n",
        "-   `Classification`: For a categorical response $Y$ with $K$ classes, a common choice is a permutation test based on the chi-squared statistic or a linear statistic derived from the conditional distribution.\n",
        "\n",
        "-   `Regression`: For a continuous response `Y`, a linear statistic (e.g., based on the correlation between $X_j$ and $Y$ or a test like the ANOVA F-test is used.\n",
        "\n",
        "The test statistic is computed as follows:\n",
        "\n",
        "-   Let $\\mathbf{X} = (X_1, X_2, \\dots, X_p)$ be the $p$ predictor variables, and $Y$ be the response.\n",
        "\n",
        "-   For a predictor $X_j$, the algorithm computes a test statistic $T_j$ that measures the association between $X_j$ and $Y$.\n",
        "\n",
        "-   The p-value for $T_j$ is calculated using a permutation test or an asymptotic distribution, adjusted for multiple testing (e.g., Bonferroni correction).\n",
        "\n",
        "For a linear statistic, the test statistic can be written as:\n",
        "\n",
        "$$ T_j = \\sum_{i=1}^n g(X_{ij}) h(Y_i) $$\n",
        "\n",
        "where:\n",
        "\n",
        "-   $g(X_{ij})$ is a transformation of the predictor $X_j$ for observation $i$ (e.g., rank or indicator function for splits).\n",
        "\n",
        "-   $h(Y_i)$ is a transformation of the response (e.g., class indicator for classification, or the response value for regression).\n",
        "\n",
        "-   The expectation and variance of $T_j$ under $H_0$ are computed conditionally on the observed data, and the p-value is derived from the standardized statistic:\n",
        "\n",
        "$$ Z_j = \\frac{T_j - E(T_j)}{\\sqrt{\\text{Var}(T_j)}} $$\n",
        "\n",
        "The p-value is then $p_j = P(|Z_j| > |z_j| \\mid H_0)$, where $z_j$ is the observed value of $Z_j$.\n",
        "\n",
        "2. Variable Selection\n",
        "\n",
        "At each node, CITs select the predictor variable $X_j$ that has the strongest association with $Y$, based on the smallest p-value from the hypothesis tests.\n",
        "\n",
        "-   Compute the test statistic $T_j$ and p-value $p_j$ for each predictor $X_j$ ($j = 1, \\dots, p$).\n",
        "\n",
        "-   Adjust the p-values for multiple testing (e.g., using Bonferroni correction: $p_j^{\\text{adj}} = \\min(p_j \\cdot p, 1)$).\n",
        "\n",
        "-   Select the variable $X_j^*$ ) with the smallest adjusted p-value, provided it is below a predefined significance threshold $\\alpha$ (e.g., $\\alpha = 0.05$):\n",
        "\n",
        "$$ j^* = \\arg\\min_j p_j^{\\text{adj}}. $$\n",
        "\n",
        "-   If no p-value is below $\\alpha$ the node is not split, and it becomes a terminal node.\n",
        "\n",
        "The decision rule is:\n",
        "\n",
        "$$ \\text{If } \\min_j p_j^{\\text{adj}} < \\alpha, \\text{ select } X_{j^*}; \\text{ else, stop splitting.} $$\n",
        "\n",
        "**Classification vs. Regression**:\n",
        "\n",
        "-   For classification, the test statistic often measures the difference in class distributions across possible splits (e.g., using a chi-squared test).\n",
        "-   For regression, the test statistic measures the difference in means or variances of ( Y ) across splits (e.g., using an F-test or correlation-based statistic).\n",
        "\n",
        "3. Split Point Selection\n",
        "\n",
        "Once the best predictor $X_j^*$ is selected, the algorithm determines the optimal split point for that variable. The split divides the node’s data into two child nodes based on a condition like $X_j^* \\leq s$ and $X_j^* > s$.\n",
        "\n",
        "-   For a continuous or ordinal predictor, evaluate all possible split points $s$.\n",
        "-   For a categorical predictor, consider all possible binary partitions of the categories.\n",
        "-   The split point is chosen to maximize the association between the split and the response $Y$, often by maximizing the test statistic or minimizing the p-value of a two-sample test.\n",
        "\n",
        "For a split point $s$, define the two regions: - Left child: $\\{i : X_{ij^*} \\leq s\\}$. - Right child: $\\{i : X_{ij^*} > s\\}$.\n",
        "\n",
        "Compute a two-sample test statistic for the response $Y$ in the two regions. For example:\n",
        "\n",
        "-   **Classification**: Use a chi-squared test to compare the class distributions in the two regions.\n",
        "-   **Regression**: Use a t-test or F-test to compare the means of $Y$ in the two regions.\n",
        "\n",
        "The optimal split point $s^*$ is:\n",
        "\n",
        "$$ s^* = \\arg\\max_s T(s),$$\n",
        "\n",
        "where $T(s)$ is the test statistic for the split at $s$. Alternatively, the split can minimize the p-value of the two-sample test.\n",
        "\n",
        "4. Recursive Partitioning\n",
        "\n",
        "After selecting the predictor $X_j^*$ and split point $s^*$, the node is split into two child nodes:\n",
        "\n",
        "-   Left child: Observations where $X_j^* \\leq s^*$.\n",
        "\n",
        "-   Right child: Observations where $X_j^* > s^*$.\n",
        "\n",
        "The algorithm then recursively applies Steps 1–3 to each child node, treating the data in each child node as a new dataset. This process continues until a stopping criterion is met.\n",
        "\n",
        "5. topping Criteria\n",
        "\n",
        "The recursion stops when one of the following conditions is satisfied:\n",
        "\n",
        "-   No predictor has an adjusted p-value below the significance threshold $\\alpha$, indicating no significant association with $Y$.\n",
        "-   The node contains fewer than a minimum number of observations (e.g., $n_{\\min}$).\n",
        "-   The maximum tree depth is reached (optional, user-specified).\n",
        "-   The response values in the node are sufficiently homogeneous (e.g., all observations belong to the same class for classification, or the variance of $Y$ is below a threshold for regression).\n",
        "\n",
        "For regression, homogeneity can be assessed using the variance of $Y$ in the node:\n",
        "\n",
        "$$ \\text{Var}(Y) = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\bar{Y})^2, $$\n",
        "\n",
        "where $\\bar{Y}$ is the mean of $Y$ in the node.\n",
        "\n",
        "If $\\text{Var}(Y)$ is small, further splitting may not be necessary.\n",
        "\n",
        "6. Prediction\n",
        "\n",
        "Once the tree is built, predictions are made as follows:\n",
        "\n",
        "-   **Classification**:\n",
        "\n",
        "-   For a new observation with features $\\mathbf{x}$, traverse the tree by following the decision rules until a terminal node is reached.\n",
        "\n",
        "-   The predicted class is the majority class in the terminal node:\n",
        "\n",
        "$$\\hat{y} = \\arg\\max_k \\frac{n_k}{n} $$\n",
        "\n",
        "where $n_k$ is the number of observations of class $k$ in the terminal node, and $n$ is the total number of observations in the node.\n",
        "\n",
        "-   **Regression**:\n",
        "\n",
        "-   Traverse the tree to reach a terminal node.\n",
        "\n",
        "-   The predicted value is the mean of the response values in the terminal node:\n",
        "\n",
        "$$ \\hat{y} = \\frac{1}{n} \\sum_{i \\in \\text{node}} Y_i. $$\n",
        "\n"
      ],
      "metadata": {
        "id": "Vn-U0DinTuIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below figure summarizes the workflow of a Conditional Inference Tree (CIT):\n",
        "\n",
        "![alt text](http://drive.google.com/uc?export=view&id=1gsrt0fg5XUnWjX8iQqylEA5TX4TXyOld)"
      ],
      "metadata": {
        "id": "-1hM2cPkdVdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Differences Between Classification and Regression\n",
        "\n",
        "| Aspect | Classification | Regression |\n",
        "|------------------------|------------------------|------------------------|\n",
        "| `Response Variable` | Categorical ($Y \\in \\{1, \\dots, K\\}$) | Continuous ($Y \\in \\mathbb{R}$) |\n",
        "| `Test Statistic` | Chi-squared, permutation test | F-test, t-test, correlation-based |\n",
        "| `Split Criterion` | Maximize class separation (e.g., Gini) | Minimize variance or maximize mean difference |\n",
        "| `Prediction` | Majority class in terminal node | Mean of response in terminal node |\n"
      ],
      "metadata": {
        "id": "RbdjXkcsd113"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of CITs\n",
        "\n",
        "-   `Unbiased Variable Selection`: Unlike CART, which favors variables with many split points, CITs use p-values to ensure fairness.\n",
        "-   `Statistical Rigor`: The use of hypothesis testing controls overfitting and provides a principled stopping rule.\n",
        "-   `Flexibility`: Works for both classification and regression, handling mixed predictor types (continuous, categorical)."
      ],
      "metadata": {
        "id": "Odu-44w7d2kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations\n",
        "\n",
        "-   `Computational Cost`: Permutation tests and p-value adjustments can be computationally intensive.\n",
        "-   `Interpretability`: Deep trees may still be complex to interpret.\n",
        "-   `Performance`: May not always outperform other methods (e.g., random forests) in predictive accuracy."
      ],
      "metadata": {
        "id": "U0RHqEeZd6qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Does a Conditional Inference Tree Differ from a Traditional Decision Tree?\n",
        "\n",
        "Conditional Inference Trees (CITs) and traditional decision trees (like CART) are both tree-based models used for classification and regression. However, they differ significantly in their approach to splitting data and handling variable selection. Below is a comparison of the two methods:\n",
        "\n",
        "| **Aspect** | **Conditional Inference Tree (CIT)** | **Traditional Decision Tree (e.g., CART)** |\n",
        "|------------------------|------------------------|------------------------|\n",
        "| `Splitting Criterion` | Uses statistical hypothesis tests (p-values) to select splits. | Uses heuristic measures like Gini impurity, entropy, or variance reduction. |\n",
        "| `Variable Selection Bias` | Unbiased; avoids favoring variables with more categories or values. | Biased toward variables with more split points (e.g., continuous or multi-level categorical variables). |\n",
        "| `Statistical Foundation` | Grounded in statistical theory, with control for multiple testing. | Heuristic-based, lacking formal statistical guarantees. |\n",
        "| `Overfitting Control` | Stops splitting when no significant association is found, reducing overfitting. | Relies on pruning or maximum depth to control overfitting, which may not be statistically justified. |\n",
        "| `Interpretability` | Similar interpretability, but splits are statistically justified. | Highly interpretable, but splits may be less reliable due to bias. |\n",
        "| `Flexibility` | Handles mixed data types well; less sensitive to data distribution. | Flexible but may require preprocessing for certain data types. |\n",
        "| `Computational Complexity` | Slightly higher due to statistical testing at each node. | Generally faster due to simpler heuristic calculations. |\n",
        "\n",
        "Below are the key differences in practice:\n",
        "\n",
        "-   `Bias in Variable Selection`: In a traditional decision tree, a categorical variable with 10 categories might be preferred over one with 2 categories because it offers more splitting options. CITs avoid this by using p-values, making variable selection fairer.\n",
        "-   `Statistical Rigor`: CITs provide a more rigorous framework, as splits are based on statistically significant associations, which can lead to more reliable models in settings where interpretability and statistical validity are crucial (e.g., medical or social sciences).\n",
        "-   `Overfitting`: CITs naturally limit tree growth by stopping when no significant splits are found, whereas traditional trees may grow excessively and require post-pruning."
      ],
      "metadata": {
        "id": "ZHVhJNzCd7O3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional Inference Tree (CIT) from Scratch in Python\n",
        "\n",
        "Conditional Inference Trees (CIT) are an advanced type of decision tree that use statistical tests to select splits, reducing bias toward features with many possible splits (unlike CART, which uses Gini or MSE). Implementing CIT from scratch in Python without packages is complex due to the need for statistical tests (e.g., permutation tests) for split selection. I’ll provide a simplified implementation of CIT in Python, focusing on core concepts while maintaining compatibility with your prior context (classification on Iris, regression on Boston Housing)."
      ],
      "metadata": {
        "id": "UdMcXPIF4Bly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Example\n",
        "\n",
        "For classification, we’ll:\n",
        "- Test each feature’s association with `Species` using a chi-squared test on discretized feature values.\n",
        "- Select the feature and threshold with the lowest p-value.\n",
        "- Recursively split until a significance threshold (e.g., p > 0.05) or other criteria (max depth, min size) are met."
      ],
      "metadata": {
        "id": "D_Me2SujiEbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data\n",
        "\n",
        "We will create a synthetic dataset that mimics the Iris dataset, ensuring that the features are numeric and the target variable is a factor. This will help us avoid issues with factor levels and ensure that the model can be trained effectively.\n"
      ],
      "metadata": {
        "id": "ztl1QBDb4MYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Synthetic dataset mimicking Iris\n",
        "n = 150\n",
        "data_class = pd.DataFrame({\n",
        "    'sepal_length': np.random.uniform(4, 8, n),\n",
        "    'sepal_width': np.random.uniform(2, 4.5, n),\n",
        "    'petal_length': np.random.uniform(1, 7, n),\n",
        "    'petal_width': np.random.uniform(0.1, 2.5, n),\n",
        "    'species': np.random.choice(['setosa', 'versicolor', 'virginica'], n, p=[0.33, 0.33, 0.34])\n",
        "})\n",
        "print(\"Classification Data Head:\")\n",
        "print(data_class.head())"
      ],
      "metadata": {
        "id": "q82MNCn64Q0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295774b8-1c9a-4369-edf0-1916874f0fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Data Head:\n",
            "   sepal_length  sepal_width  petal_length  petal_width    species\n",
            "0      4.985284     4.241064      5.188352     2.066693  virginica\n",
            "1      5.541086     2.558176      1.000721     1.313207  virginica\n",
            "2      5.120000     2.670311      5.930882     2.417035  virginica\n",
            "3      6.630641     2.486245      3.137439     1.679414  virginica\n",
            "4      5.296886     4.418753      5.467719     1.523771  virginica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define CIT Functions for Classification\n",
        "\n",
        "1. `discretize`: Converts a numeric feature into discrete bins using quantiles.\n",
        "   - Checks for sufficient unique values; if too few, returns a factor or single bin.\n",
        "   - Creates bins based on quantiles, adjusting breaks if needed, and handles NAs by assigning them to bin 1.\n",
        "\n",
        "2. `chi_squared_test`: Tests association between a feature and target using a chi-squared test.\n",
        "   - Skips if too few unique values or invalid contingency table (e.g., zeros or small counts).\n",
        "   - Returns p-value; defaults to 1 if test fails or returns NA.\n",
        "\n",
        "3. `find_best_split_class`: Identifies the best binary split for numeric features.\n",
        "   - Iterates over numeric features and possible thresholds (midpoints of sorted values).\n",
        "   - Discretizes feature at each threshold, tests with chi-squared, and tracks split with lowest p-value.\n",
        "   - Returns feature, threshold, p-value, and indices for left/right splits; returns NULL if no valid split.\n",
        "\n",
        "4. `build_tree_class`: Recursively builds a classification tree.\n",
        "   - Stops if: single class, too few rows, or max depth reached; returns a leaf with majority class.\n",
        "   - Finds best split; if none or p-value > alpha, returns a leaf.\n",
        "   - Splits data and recursively builds left/right subtrees, returning a node with feature, threshold, and subtrees.\n",
        "\n",
        "5. `predict_tree_class`: Predicts class for a single row using the tree.\n",
        "   - Returns leaf's class if at a leaf.\n",
        "   - Traverses left or right subtree based on feature value relative to threshold (handles NAs by going left).\n"
      ],
      "metadata": {
        "id": "4BpOoaj34eBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency, f_oneway\n",
        "import statsmodels.api as sm\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "# Discretize a numeric feature into bins\n",
        "def discretize(x: np.ndarray, breaks: int = 4) -> np.ndarray:\n",
        "    x = np.array(x)\n",
        "    valid_x = x[~np.isnan(x)]\n",
        "    if len(np.unique(valid_x)) < 2:\n",
        "        print(\"Discretize: too few unique values, returning as is\")\n",
        "        return x.astype(int)\n",
        "    n_unique = len(np.unique(valid_x))\n",
        "    if n_unique < breaks:\n",
        "        breaks = n_unique\n",
        "    if breaks < 2:\n",
        "        print(\"Discretize: forcing single bin\")\n",
        "        return np.ones(len(x), dtype=int)\n",
        "    quantiles = np.nanquantile(x, np.linspace(0, 1, breaks + 1))\n",
        "    quantiles = np.unique(quantiles)\n",
        "    if len(quantiles) < 2:\n",
        "        print(\"Discretize: quantiles collapsed, forcing single bin\")\n",
        "        return np.ones(len(x), dtype=int)\n",
        "    if len(quantiles) < breaks + 1:\n",
        "        breaks = len(quantiles) - 1\n",
        "        print(f\"Discretize: reduced breaks to {breaks}\")\n",
        "    result = pd.cut(x, bins=quantiles, labels=False, include_lowest=True)\n",
        "    result = np.array(result, dtype=float)\n",
        "    result[np.isnan(result)] = 1\n",
        "    return result.astype(int)\n",
        "\n",
        "# Chi-squared test for feature-target association\n",
        "def chi_squared_test(feature: np.ndarray, target: np.ndarray) -> float:\n",
        "    if len(np.unique(feature)) < 2 or len(np.unique(target)) < 2:\n",
        "        print(\"Skipping chi-squared test: too few unique values\")\n",
        "        return 1.0\n",
        "    table = pd.crosstab(feature, target)\n",
        "    if table.shape[0] < 2 or table.shape[1] < 2 or (table == 0).any().any() or table.sum().sum() < 10:\n",
        "        print(\"Invalid contingency table:\")\n",
        "        print(table)\n",
        "        return 1.0\n",
        "    try:\n",
        "        _, pvalue, _, _ = chi2_contingency(table, correction=False)\n",
        "        if np.isnan(pvalue):\n",
        "            print(\"Chi-squared test returned NA\")\n",
        "            return 1.0\n",
        "        return pvalue\n",
        "    except:\n",
        "        print(\"Chi-squared test failed\")\n",
        "        return 1.0\n",
        "\n",
        "# Find best split for classification\n",
        "def find_best_split_class(data: pd.DataFrame, target: str) -> Dict:\n",
        "    features = [col for col in data.columns if col != target]\n",
        "    numeric_features = [col for col in features if np.issubdtype(data[col].dtype, np.number)]\n",
        "\n",
        "    best_pvalue = 1.0\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "    best_split = None\n",
        "\n",
        "    for feature in numeric_features:\n",
        "        values = np.sort(data[feature].dropna().unique())\n",
        "        if len(values) < 2:\n",
        "            print(f\"Skipping feature {feature}: too few unique values\")\n",
        "            continue\n",
        "        thresholds = (values[:-1] + values[1:]) / 2\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            left_idx = (data[feature] <= threshold) & (~data[feature].isna())\n",
        "            right_idx = (data[feature] > threshold) & (~data[feature].isna())\n",
        "            if left_idx.sum() < 5 or right_idx.sum() < 5:\n",
        "                print(f\"Skipping threshold {threshold} for {feature}: too few observations\")\n",
        "                continue\n",
        "\n",
        "            split_feature = discretize((data[feature] <= threshold).astype(int))\n",
        "            pvalue = chi_squared_test(split_feature, data[target])\n",
        "\n",
        "            if np.isnan(pvalue):\n",
        "                print(f\"NA p-value for feature {feature} at threshold {threshold}\")\n",
        "                continue\n",
        "\n",
        "            if pvalue < best_pvalue:\n",
        "                best_pvalue = pvalue\n",
        "                best_feature = feature\n",
        "                best_threshold = threshold\n",
        "                best_split = {'left_idx': left_idx, 'right_idx': right_idx}\n",
        "\n",
        "    if best_feature is None:\n",
        "        print(\"No valid split found\")\n",
        "\n",
        "    return {\n",
        "        'feature': best_feature,\n",
        "        'threshold': best_threshold,\n",
        "        'pvalue': best_pvalue,\n",
        "        'split': best_split\n",
        "    }\n",
        "\n",
        "# Build conditional inference tree for classification\n",
        "def build_tree_class(data: pd.DataFrame, target: str, max_depth: int = 3, min_size: int = 5,\n",
        "                     depth: int = 0, alpha: float = 0.05) -> Dict:\n",
        "    classes = data[target]\n",
        "\n",
        "    # Stopping criteria\n",
        "    if len(classes.unique()) == 1 or len(data) < min_size or depth >= max_depth:\n",
        "        return {\n",
        "            'is_leaf': True,\n",
        "            'class': classes.mode()[0]\n",
        "        }\n",
        "\n",
        "    # Find best split\n",
        "    split_info = find_best_split_class(data, target)\n",
        "    if split_info['feature'] is None or split_info['pvalue'] > alpha:\n",
        "        return {\n",
        "            'is_leaf': True,\n",
        "            'class': classes.mode()[0]\n",
        "        }\n",
        "\n",
        "    # Split data\n",
        "    left_data = data[split_info['split']['left_idx']]\n",
        "    right_data = data[split_info['split']['right_idx']]\n",
        "\n",
        "    # Build subtrees\n",
        "    left_subtree = build_tree_class(left_data, target, max_depth, min_size, depth + 1, alpha)\n",
        "    right_subtree = build_tree_class(right_data, target, max_depth, min_size, depth + 1, alpha)\n",
        "\n",
        "    return {\n",
        "        'is_leaf': False,\n",
        "        'feature': split_info['feature'],\n",
        "        'threshold': split_info['threshold'],\n",
        "        'left': left_subtree,\n",
        "        'right': right_subtree\n",
        "    }\n",
        "\n",
        "# Predict for a single row (classification)\n",
        "def predict_tree_class(tree: Dict, row: pd.Series) -> str:\n",
        "    if tree['is_leaf']:\n",
        "        return tree['class']\n",
        "    if pd.isna(row[tree['feature']]) or row[tree['feature']] <= tree['threshold']:\n",
        "        return predict_tree_class(tree['left'], row)\n",
        "    else:\n",
        "        return predict_tree_class(tree['right'], row)\n"
      ],
      "metadata": {
        "id": "6C-2iM4ju9qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the Model\n"
      ],
      "metadata": {
        "id": "vJ08RKtk5RRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and evaluate classification model\n",
        "n = len(data_class)\n",
        "train_idx = np.random.choice(n, int(0.8 * n), replace=False)\n",
        "train_data = data_class.iloc[train_idx]\n",
        "test_data = data_class.drop(train_idx)\n",
        "\n",
        "# Ensure no NA in target\n",
        "train_data = train_data[train_data['species'].notna()]\n",
        "test_data = test_data[test_data['species'].notna()]\n",
        "\n",
        "# Build tree\n",
        "tree_class = build_tree_class(train_data, 'species', max_depth=3, min_size=5, alpha=0.05)\n"
      ],
      "metadata": {
        "id": "uRm4jHSs5d2s",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction and Evaluation"
      ],
      "metadata": {
        "id": "DZb2rdMl5kZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "predictions = test_data.apply(lambda row: predict_tree_class(tree_class, row), axis=1)\n",
        "\n",
        "# Check for NA in predictions\n",
        "if predictions.isna().any():\n",
        "    print(\"Warning: NA values in predictions\")\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = (predictions == test_data['species']).mean()\n",
        "print(f\"Classification Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "i3CEmPnMV2Gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95223d52-dfd9-405e-db68-d3abaa2f1863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy: 0.26666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Example\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VRGuENR1V7m7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data\n",
        "\n",
        "We will create a synthetic dataset that mimics the Boston Housing dataset, ensuring that the `rooms` feature is numeric and `medv` is calculated after the data frame is created. This will help us avoid issues with factor levels and ensure that the model can be trained effectively."
      ],
      "metadata": {
        "id": "f90W5DkzkaR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetic dataset mimicking Boston Housing\n",
        "n = 506\n",
        "data_reg = pd.DataFrame({\n",
        "    'crim': np.random.uniform(0, 90, n),\n",
        "    'zn': np.random.choice([0, 18, 25, 35], n, p=[0.7, 0.1, 0.1, 0.1]),\n",
        "    'indus': np.random.uniform(0, 27, n),\n",
        "    'chas': np.random.choice([0, 1], n, p=[0.9, 0.1]),\n",
        "    'nox': np.random.uniform(0.38, 0.87, n),\n",
        "    'rooms': np.random.uniform(3, 9, n),\n",
        "    'age': np.random.uniform(0, 100, n),\n",
        "    'dis': np.random.uniform(1, 12, n),\n",
        "    'rad': np.random.choice(np.arange(1, 25), n),\n",
        "    'tax': np.random.uniform(187, 711, n),\n",
        "    'ptratio': np.random.uniform(12, 22, n),\n",
        "    'b': np.random.uniform(0, 396, n),\n",
        "    'lstat': np.random.uniform(1, 37, n)\n",
        "})\n",
        "data_reg['medv'] = 20 + 0.1 * (8 - data_reg['rooms']) + 0.2 * data_reg['lstat'] + \\\n",
        "                   0.05 * data_reg['crim'] + np.random.normal(0, 5, n)\n",
        "data_reg['medv'] = np.clip(data_reg['medv'], 5, 50)\n",
        "\n",
        "# Verify rooms is numeric\n",
        "if not np.issubdtype(data_reg['rooms'].dtype, np.number):\n",
        "    data_reg['rooms'] = pd.to_numeric(data_reg['rooms'], errors='raise')\n",
        "\n",
        "print(\"\\nRegression Data Structure:\")\n",
        "print(data_reg.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSwOo3aPkd8_",
        "outputId": "139b8208-ac7e-4c92-b345-46c7429a10ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Regression Data Structure:\n",
            "crim       float64\n",
            "zn           int64\n",
            "indus      float64\n",
            "chas         int64\n",
            "nox        float64\n",
            "rooms      float64\n",
            "age        float64\n",
            "dis        float64\n",
            "rad          int64\n",
            "tax        float64\n",
            "ptratio    float64\n",
            "b          float64\n",
            "lstat      float64\n",
            "medv       float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define CIT Functions for Regression\n",
        "\n",
        "\n",
        "1. `f_test`: Tests association between a feature and numeric target using an F-test via linear regression.\n",
        "   - Skips if too few unique values or groups < 2 levels; returns 1 if test fails.\n",
        "   - Returns p-value from ANOVA of the regression model.\n",
        "\n",
        "2. `find_best_split_reg`: Finds the best binary split for numeric features in regression.\n",
        "   - Iterates over features and thresholds (midpoints of sorted values).\n",
        "   - Tests splits using f_test, tracking the split with the lowest p-value.\n",
        "   - Returns feature, threshold, p-value, and split indices; returns NULL if no valid split.\n",
        "\n",
        "3. `build_tree_reg`: Recursively builds a regression tree.\n",
        "   - Stops if too few rows, max depth reached, or all target values are NA; returns a leaf with mean target value (0 if all NA).\n",
        "   - Uses best split if p-value < alpha; otherwise, returns a leaf.\n",
        "   - Splits data and builds left/right subtrees.\n",
        "\n",
        "4. `predict_tree_reg`: Predicts a numeric value for a single row.\n",
        "   - Returns leaf’s value (0 if NA) or traverses left/right subtree based on feature value vs. threshold (NAs go left)."
      ],
      "metadata": {
        "id": "h-hvjdXIkxW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# F-test for feature-target association\n",
        "def f_test(feature: np.ndarray, target: np.ndarray) -> float:\n",
        "    if len(np.unique(feature)) < 2 or len(np.unique(target)) < 2:\n",
        "        print(\"Skipping F-test: too few unique values\")\n",
        "        return 1.0\n",
        "    groups = pd.Series(feature).astype(str)\n",
        "    if len(groups.unique()) < 2:\n",
        "        print(\"F-test: groups have fewer than 2 levels\")\n",
        "        return 1.0\n",
        "    try:\n",
        "        group_values = [target[groups == g] for g in groups.unique()]\n",
        "        f_stat, pvalue = f_oneway(*group_values)\n",
        "        if np.isnan(pvalue):\n",
        "            print(\"F-test returned NA\")\n",
        "            return 1.0\n",
        "        return pvalue\n",
        "    except:\n",
        "        print(\"F-test failed\")\n",
        "        return 1.0\n",
        "\n",
        "# Find best split for regression\n",
        "def find_best_split_reg(data: pd.DataFrame, target: str) -> Dict:\n",
        "    features = [col for col in data.columns if col != target]\n",
        "    numeric_features = [col for col in features if np.issubdtype(data[col].dtype, np.number)]\n",
        "\n",
        "    best_pvalue = 1.0\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "    best_split = None\n",
        "\n",
        "    for feature in numeric_features:\n",
        "        values = np.sort(data[feature].dropna().unique())\n",
        "        if len(values) < 2:\n",
        "            print(f\"Skipping feature {feature}: too few unique values\")\n",
        "            continue\n",
        "        thresholds = (values[:-1] + values[1:]) / 2\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            left_idx = (data[feature] <= threshold) & (~data[feature].isna())\n",
        "            right_idx = (data[feature] > threshold) & (~data[feature].isna())\n",
        "            if left_idx.sum() < 5 or right_idx.sum() < 5:\n",
        "                print(f\"Skipping threshold {threshold} for {feature}: too few observations\")\n",
        "                continue\n",
        "\n",
        "            split_feature = (data[feature] <= threshold).astype(int)\n",
        "            pvalue = f_test(split_feature, data[target])\n",
        "\n",
        "            if np.isnan(pvalue):\n",
        "                print(f\"NA p-value for feature {feature} at threshold {threshold}\")\n",
        "                continue\n",
        "\n",
        "            if pvalue < best_pvalue:\n",
        "                best_pvalue = pvalue\n",
        "                best_feature = feature\n",
        "                best_threshold = threshold\n",
        "                best_split = {'left_idx': left_idx, 'right_idx': right_idx}\n",
        "\n",
        "    if best_feature is None:\n",
        "        print(\"No valid split found\")\n",
        "\n",
        "    return {\n",
        "        'feature': best_feature,\n",
        "        'threshold': best_threshold,\n",
        "        'pvalue': best_pvalue,\n",
        "        'split': best_split\n",
        "    }\n",
        "\n",
        "# Build conditional inference tree for regression\n",
        "def build_tree_reg(data: pd.DataFrame, target: str, max_depth: int = 3, min_size: int = 5,\n",
        "                   depth: int = 0, alpha: float = 0.05) -> Dict:\n",
        "    target_values = data[target]\n",
        "\n",
        "    # Stopping criteria\n",
        "    if len(data) < min_size or depth >= max_depth or target_values.isna().all():\n",
        "        return {\n",
        "            'is_leaf': True,\n",
        "            'value': target_values.mean() if not target_values.isna().all() else 0\n",
        "        }\n",
        "\n",
        "    # Find best split\n",
        "    split_info = find_best_split_reg(data, target)\n",
        "    if split_info['feature'] is None or split_info['pvalue'] > alpha:\n",
        "        return {\n",
        "            'is_leaf': True,\n",
        "            'value': target_values.mean() if not target_values.isna().all() else 0\n",
        "        }\n",
        "\n",
        "    # Split data\n",
        "    left_data = data[split_info['split']['left_idx']]\n",
        "    right_data = data[split_info['split']['right_idx']]\n",
        "\n",
        "    # Build subtrees\n",
        "    left_subtree = build_tree_reg(left_data, target, max_depth, min_size, depth + 1, alpha)\n",
        "    right_subtree = build_tree_reg(right_data, target, max_depth, min_size, depth + 1, alpha)\n",
        "\n",
        "    return {\n",
        "        'is_leaf': False,\n",
        "        'feature': split_info['feature'],\n",
        "        'threshold': split_info['threshold'],\n",
        "        'left': left_subtree,\n",
        "        'right': right_subtree\n",
        "    }\n",
        "\n",
        "# Predict for a single row (regression)\n",
        "def predict_tree_reg(tree: Dict, row: pd.Series) -> float:\n",
        "    if tree['is_leaf']:\n",
        "        return tree['value'] if not np.isnan(tree['value']) else 0\n",
        "    if pd.isna(row[tree['feature']]) or row[tree['feature']] <= tree['threshold']:\n",
        "        return predict_tree_reg(tree['left'], row)\n",
        "    else:\n",
        "        return predict_tree_reg(tree['right'], row)"
      ],
      "metadata": {
        "id": "fftqCz53kyLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the Model"
      ],
      "metadata": {
        "id": "JEKvE-ztk5Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and evaluate regression model\n",
        "n = len(data_reg)\n",
        "train_idx = np.random.choice(n, int(0.8 * n), replace=False)\n",
        "train_data = data_reg.iloc[train_idx]\n",
        "test_data = data_reg.drop(train_idx)\n",
        "\n",
        "# Ensure no NA in target\n",
        "train_data = train_data[train_data['medv'].notna()]\n",
        "test_data = test_data[test_data['medv'].notna()]\n",
        "\n",
        "# Build tree\n",
        "tree_reg = build_tree_reg(train_data, 'medv', max_depth=3, min_size=5, alpha=0.05)"
      ],
      "metadata": {
        "id": "21Nex-DiwBq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction and Evaluation"
      ],
      "metadata": {
        "id": "SBnYQbLQlJjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "predictions = test_data.apply(lambda row: predict_tree_reg(tree_reg, row), axis=1)\n",
        "\n",
        "# Check for NA in predictions\n",
        "if predictions.isna().any():\n",
        "    print(\"Warning: NA values in predictions\")\n",
        "\n",
        "# Evaluate MSE\n",
        "mse = ((predictions - test_data['medv']) ** 2).mean()\n",
        "print(f\"Regression MSE: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_Lk4k1glJtd",
        "outputId": "f72e6d62-0fda-497b-8403-24781e256390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression MSE: 28.92953709373793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional Inference Trees in Python\n",
        "\n",
        "Conditional Inference Trees (CITs) are decision trees that use statistical hypothesis testing (e.g., chi-squared for classification, F-tests for regression) to select unbiased splits, reducing overfitting compared to traditional CART methods. They recursively partition data based on p-values, stopping when no significant splits are found. In Python, there's no direct equivalent to R's `partykit::ctree`, but you can approximate CITs using `scikit-learn's` `DecisionTreeClassifier` or `DecisionTreeRegressor` with custom split logic incorporating statistical tests from `scipy.stats`. Below is a concise example for classification and regression."
      ],
      "metadata": {
        "id": "sZM4h2ad5q78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Problem\n",
        "\n",
        "Below is a concise example for classification on the Iris dataset."
      ],
      "metadata": {
        "id": "JPBHJCs6oo8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.datasets import load_iris"
      ],
      "metadata": {
        "id": "R1N29VJnjQao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data\n",
        "\n",
        "We will use the `Iris` dataset, which contains measurements of iris flowers and their species. The dataset has four features (sepal length, sepal width, petal length, and petal width) and three classes (setosa, versicolor, and virginica).\n"
      ],
      "metadata": {
        "id": "rSJqB05rO5ES"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFpt_H01OPsa"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "data['Species'] = iris.target_names[iris.target]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('Species', axis=1), data['Species'], train_size=0.7, random_state=123\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Custom CIT splitter"
      ],
      "metadata": {
        "id": "vjia0yAnjvhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code finds the best way to split your data for a classification tree. It checks each feature and possible split point, using a chi-squared test to find the split that's most related to the target variable (the one with the lowest p-value). It then returns the best feature, the value to split on, and how statistically significant that split is."
      ],
      "metadata": {
        "id": "xefqMi6jjmUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom CIT splitter\n",
        "def cit_splitter(X, y):\n",
        "    best_pvalue = 1.0\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "    for feature in X.columns:\n",
        "        values = np.sort(X[feature].dropna().unique())\n",
        "        if len(values) < 2:\n",
        "            continue\n",
        "        thresholds = (values[:-1] + values[1:]) / 2\n",
        "        for threshold in thresholds:\n",
        "            split = (X[feature] <= threshold).astype(int)\n",
        "            table = pd.crosstab(split, y)\n",
        "            if table.shape[0] < 2 or table.shape[1] < 2 or (table == 0).any().any():\n",
        "                continue\n",
        "            _, pvalue, _, _ = chi2_contingency(table, correction=False)\n",
        "            if pvalue < best_pvalue:\n",
        "                best_pvalue = pvalue\n",
        "                best_feature = feature\n",
        "                best_threshold = threshold\n",
        "    return best_feature, best_threshold, best_pvalue"
      ],
      "metadata": {
        "id": "d1UMushFjY3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit Conditional Inference Tree (CIT)\n",
        "\n",
<<<<<<< HEAD
        "Main function of `DecisionTreeClassifier()`, which implements CITs for both classificationtasks. The function uses a statistical framework to select splits based on p-values, ensuring unbiased variable selection and robust stopping criteria."
      ]
=======
        "Main function of `DecisionTreeClassifier()`, which implements CITs for both classificationtasks. The function uses a statistical framework to select splits based on p-values, ensuring unbiased variable selection and robust stopping criteria.."
      ],
      "metadata": {
        "id": "kK6jj2ZtBcNP"
      }
>>>>>>> e0811172cad685ffb6420aa1dda783a8b6b282df
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit CIT-like model\n",
        "model = DecisionTreeClassifier(max_depth=3, min_samples_split=5, random_state=123)\n",
        "best_feature, best_threshold, best_pvalue = cit_splitter(X_train, y_train)\n",
        "if best_pvalue < 0.05:  # Mimic mincriterion = 0.95\n",
        "    model.fit(X_train, y_train)\n",
        "else:\n",
        "    print(\"No significant split found\")"
      ],
      "metadata": {
        "id": "TuIIZ-40BdCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction and Evaluation"
      ],
      "metadata": {
        "id": "iczQZefmk5G2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"CIT Classification Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh_NHIpHlf_P",
        "outputId": "e14b332f-de0c-40c8-d678-95515bc4c03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIT Classification Accuracy: 0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
<<<<<<< HEAD
      "metadata": {
        "id": "US6MkVbrqqKF"
      },
=======
      "source": [
        "#### CIT with Pruning Tree\n",
        "\n",
        "CITs are self-pruning via mincriterion; inspect tree size. If too complex, increase mincriterion (e.g., 0.99), but default is usually sufficient. Here, we keep default as tree is typically small for Iris"
      ],
      "metadata": {
        "id": "rVbEM7oYqctM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import chi2_contingency\n",
        "import plotly.graph_objects as go\n",
        "import graphviz\n",
        "from sklearn.datasets import load_iris\n"
      ],
      "metadata": {
        "id": "pe3L_4kSmrnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris data\n",
        "iris = load_iris()\n",
        "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "data['Species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
        "\n",
        "# Split data\n",
        "train_idx, test_idx = train_test_split(range(len(data)), train_size=0.7, random_state=123)\n",
        "train_data = data.iloc[train_idx]\n",
        "test_data = data.iloc[test_idx]"
      ],
      "metadata": {
        "id": "sptyml_el---"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom CIT splitter\n",
        "def custom_cit_splitter(X, y, mincriterion=0.95):\n",
        "    best_pvalue = 1.0\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "    for feature in X.columns:\n",
        "        values = np.sort(X[feature].dropna().unique())\n",
        "        if len(values) < 2:\n",
        "            continue\n",
        "        thresholds = (values[:-1] + values[1:]) / 2\n",
        "        for threshold in thresholds:\n",
        "            split = (X[feature] <= threshold).astype(int)\n",
        "            table = pd.crosstab(split, y)\n",
        "            if table.shape[0] < 2 or table.shape[1] < 2 or (table == 0).any().any():\n",
        "                continue\n",
        "            _, pvalue, _, _ = chi2_contingency(table, correction=False)\n",
        "            if pvalue < best_pvalue:\n",
        "                best_pvalue = pvalue\n",
        "                best_feature = feature\n",
        "                best_threshold = threshold\n",
        "    return best_feature, best_threshold, best_pvalue"
      ],
      "metadata": {
        "id": "gRMDoqLomI6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIT Classifier with pruning\n",
        "class CITClassifier:\n",
        "    def __init__(self, mincriterion=0.95, max_depth=3, min_samples_split=5):\n",
        "        self.mincriterion = mincriterion\n",
        "        self.tree = DecisionTreeClassifier(\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            random_state=123\n",
        "        )\n",
        "        self.custom_splits = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        best_feature, best_threshold, best_pvalue = custom_cit_splitter(X, y)\n",
        "        if best_pvalue > 1 - self.mincriterion:\n",
        "            self.tree.fit(X, y)  # Fallback to simple tree\n",
        "        else:\n",
        "            self.custom_splits.append((best_feature, best_threshold, best_pvalue))\n",
        "            self.tree.fit(X, y)\n",
        "\n",
        "        # Prune tree using cost-complexity pruning\n",
        "        path = self.tree.cost_complexity_pruning_path(X, y)\n",
        "        ccp_alphas = path.ccp_alphas\n",
        "        node_counts = []\n",
        "        for ccp_alpha in ccp_alphas:\n",
        "            tree = DecisionTreeClassifier(\n",
        "                max_depth=self.tree.max_depth,\n",
        "                min_samples_split=self.tree.min_samples_split,\n",
        "                random_state=123,\n",
        "                ccp_alpha=ccp_alpha\n",
        "            )\n",
        "            tree.fit(X, y)\n",
        "            node_counts.append(tree.tree_.node_count)\n",
        "\n",
        "        # Select ccp_alpha to keep tree size reasonable (e.g., <= 10 nodes, similar to CIT)\n",
        "        target_nodes = 10\n",
        "        ccp_alpha = ccp_alphas[np.argmin(np.abs(np.array(node_counts) - target_nodes))]\n",
        "        self.tree = DecisionTreeClassifier(\n",
        "            max_depth=self.tree.max_depth,\n",
        "            min_samples_split=self.tree.min_samples_split,\n",
        "            random_state=123,\n",
        "            ccp_alpha=ccp_alpha\n",
        "        )\n",
        "        self.tree.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def get_n_nodes(self):\n",
        "        return self.tree.tree_.node_count\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.tree.predict(X)"
      ],
      "metadata": {
        "id": "YFA0Ohiuqk_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "cit_model = CITClassifier(mincriterion=0.95, max_depth=3, min_samples_split=5)\n",
        "cit_model.fit(train_data.drop('Species', axis=1), train_data['Species'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzC959Q1l3O8",
        "outputId": "f5fff11f-2457-47e6-ae75-eb77d0948087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.CITClassifier at 0x791c275cff90>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prune tree (check number of nodes)\n",
        "print(\"CIT Number of Nodes:\", cit_model.get_n_nodes())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb6lIw0nmWN3",
        "outputId": "d4539789-893c-4e0c-fe4a-332f9fdf8ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIT Number of Nodes: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
>>>>>>> e0811172cad685ffb6420aa1dda783a8b6b282df
      "source": [
        "#### Plot Tree"
      ],
      "metadata": {
        "id": "US6MkVbrqqKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "\n",
        "# Plot tree with graphviz\n",
        "dot_data = export_graphviz(\n",
        "    model.tree,\n",
        "    out_file=None,\n",
        "    feature_names=train_data.drop('Species', axis=1).columns,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "graph = graphviz.Source(dot_data)\n",
        "#graph.render(\"cit_iris_tree\", format=\"png\", view=False)\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "6G6N6afYqqwI",
        "outputId": "0b86e4ad-c5f1-4ab1-b6e7-f6b5ad40736b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"555pt\" height=\"433pt\"\n viewBox=\"0.00 0.00 555.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-429 551,-429 551,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ecfcf3\" stroke=\"black\" d=\"M263,-425C263,-425 128,-425 128,-425 122,-425 116,-419 116,-413 116,-413 116,-354 116,-354 116,-348 122,-342 128,-342 128,-342 263,-342 263,-342 269,-342 275,-348 275,-354 275,-354 275,-413 275,-413 275,-419 269,-425 263,-425\"/>\n<text text-anchor=\"start\" x=\"124\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"160\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.663</text>\n<text text-anchor=\"start\" x=\"150.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 105</text>\n<text text-anchor=\"start\" x=\"137.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [32, 40, 33]</text>\n<text text-anchor=\"start\" x=\"143\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M165,-298.5C165,-298.5 72,-298.5 72,-298.5 66,-298.5 60,-292.5 60,-286.5 60,-286.5 60,-242.5 60,-242.5 60,-236.5 66,-230.5 72,-230.5 72,-230.5 165,-230.5 165,-230.5 171,-230.5 177,-236.5 177,-242.5 177,-242.5 177,-286.5 177,-286.5 177,-292.5 171,-298.5 165,-298.5\"/>\n<text text-anchor=\"start\" x=\"90.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"77.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\n<text text-anchor=\"start\" x=\"68\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [32, 0, 0]</text>\n<text text-anchor=\"start\" x=\"75\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M168.79,-341.91C161.38,-330.65 153.33,-318.42 145.88,-307.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"148.75,-305.1 140.33,-298.67 142.9,-308.94 148.75,-305.1\"/>\n<text text-anchor=\"middle\" x=\"135.28\" y=\"-319.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#dcfae9\" stroke=\"black\" d=\"M337.5,-306C337.5,-306 207.5,-306 207.5,-306 201.5,-306 195.5,-300 195.5,-294 195.5,-294 195.5,-235 195.5,-235 195.5,-229 201.5,-223 207.5,-223 207.5,-223 337.5,-223 337.5,-223 343.5,-223 349.5,-229 349.5,-235 349.5,-235 349.5,-294 349.5,-294 349.5,-300 343.5,-306 337.5,-306\"/>\n<text text-anchor=\"start\" x=\"203.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"237\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.495</text>\n<text text-anchor=\"start\" x=\"231.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 73</text>\n<text text-anchor=\"start\" x=\"218\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 40, 33]</text>\n<text text-anchor=\"start\" x=\"220\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M222.21,-341.91C228.01,-333.1 234.2,-323.7 240.18,-314.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"243.26,-316.3 245.83,-306.02 237.41,-312.45 243.26,-316.3\"/>\n<text text-anchor=\"middle\" x=\"250.88\" y=\"-326.81\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#48e78b\" stroke=\"black\" d=\"M252,-187C252,-187 117,-187 117,-187 111,-187 105,-181 105,-175 105,-175 105,-116 105,-116 105,-110 111,-104 117,-104 117,-104 252,-104 252,-104 258,-104 264,-110 264,-116 264,-116 264,-175 264,-175 264,-181 258,-187 252,-187\"/>\n<text text-anchor=\"start\" x=\"113\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.35</text>\n<text text-anchor=\"start\" x=\"149\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.133</text>\n<text text-anchor=\"start\" x=\"143.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 42</text>\n<text text-anchor=\"start\" x=\"134\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 39, 3]</text>\n<text text-anchor=\"start\" x=\"132\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M241.97,-222.91C235.21,-213.92 227.98,-204.32 221.02,-195.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"223.79,-192.91 214.98,-187.02 218.19,-197.12 223.79,-192.91\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#8540e6\" stroke=\"black\" d=\"M429,-187C429,-187 294,-187 294,-187 288,-187 282,-181 282,-175 282,-175 282,-116 282,-116 282,-110 288,-104 294,-104 294,-104 429,-104 429,-104 435,-104 441,-110 441,-116 441,-116 441,-175 441,-175 441,-181 435,-187 429,-187\"/>\n<text text-anchor=\"start\" x=\"290\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"326\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.062</text>\n<text text-anchor=\"start\" x=\"320.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 31</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 30]</text>\n<text text-anchor=\"start\" x=\"313\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M303.38,-222.91C310.21,-213.92 317.52,-204.32 324.57,-195.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"327.41,-197.1 330.68,-187.02 321.84,-192.86 327.41,-197.1\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#3ee684\" stroke=\"black\" d=\"M109,-68C109,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,0 12,0 12,0 109,0 109,0 115,0 121,-6 121,-12 121,-12 121,-56 121,-56 121,-62 115,-68 109,-68\"/>\n<text text-anchor=\"start\" x=\"25\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.049</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 40</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 39, 1]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M138.33,-103.73C127.69,-94.33 116.38,-84.35 105.78,-74.99\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"108.02,-72.3 98.21,-68.3 103.39,-77.54 108.02,-72.3\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-68C240,-68 151,-68 151,-68 145,-68 139,-62 139,-56 139,-56 139,-12 139,-12 139,-6 145,0 151,0 151,0 240,0 240,0 246,0 252,-6 252,-12 252,-12 252,-56 252,-56 252,-62 246,-68 240,-68\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M188.6,-103.73C189.43,-95.43 190.31,-86.67 191.15,-78.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.64,-78.6 192.15,-68.3 187.67,-77.9 194.64,-78.6\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M400,-68C400,-68 303,-68 303,-68 297,-68 291,-62 291,-56 291,-56 291,-12 291,-12 291,-6 297,0 303,0 303,0 400,0 400,0 406,0 412,-6 412,-12 412,-12 412,-56 412,-56 412,-62 406,-68 400,-68\"/>\n<text text-anchor=\"start\" x=\"323.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"314\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"304.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 1]</text>\n<text text-anchor=\"start\" x=\"299\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M357.78,-103.73C357.02,-95.43 356.22,-86.67 355.45,-78.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"358.94,-77.94 354.54,-68.3 351.97,-78.58 358.94,-77.94\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M535,-68C535,-68 442,-68 442,-68 436,-68 430,-62 430,-56 430,-56 430,-12 430,-12 430,-6 436,0 442,0 442,0 535,0 535,0 541,0 547,-6 547,-12 547,-12 547,-56 547,-56 547,-62 541,-68 535,-68\"/>\n<text text-anchor=\"start\" x=\"460.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"447.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 29</text>\n<text text-anchor=\"start\" x=\"438\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 29]</text>\n<text text-anchor=\"start\" x=\"440\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 6&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>6&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M408.79,-103.73C419.69,-94.33 431.26,-84.35 442.12,-74.99\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"444.59,-77.48 449.88,-68.3 440.02,-72.18 444.59,-77.48\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x791c274dadd0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction and Evaluation"
      ],
      "metadata": {
        "id": "YRggOIGUqxeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate\n",
        "predictions = cit_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"CIT Classification Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r67S1_afqyfQ",
        "outputId": "fd8e8276-b667-439d-9700-35b6559fdf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIT Classification Accuracy: 0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Variable Importance\n"
      ],
      "metadata": {
        "id": "Rvqt4gIoBiip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate variable importance\n",
        "perm_importance = permutation_importance(\n",
        "    cit_model.tree,\n",
        "    train_data.drop('Species', axis=1),\n",
        "    train_data['Species'],\n",
        "    n_repeats=10,\n",
        "    random_state=123\n",
        ")\n",
        "\n",
        "# Create data frame for variable importance\n",
        "var_imp_cit = pd.DataFrame({\n",
        "    'Variable': train_data.drop('Species', axis=1).columns,\n",
        "    'Importance': perm_importance.importances_mean\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "var_imp_cit = var_imp_cit.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot variable importance\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.barplot(data=var_imp_cit, x='Importance', y='Variable')\n",
        "plt.title('Variable Importance for CIT-Classification')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Variable')\n",
        "plt.tight_layout()\n",
        "#plt.savefig('cit_var_importance.png')\n",
        "#plt.close()"
      ],
      "metadata": {
        "id": "yGNM8SyaBjRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "6bf94d26-7bcd-462a-ba00-8787d0714494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Problems\n",
        "\n",
        "Conditional Inference Trees (CITs) for regression work by recursively partitioning data based on statistical tests of independence, ensuring unbiased variable selection and robust stopping criteria. In this section, we will discuss the Conditional Inference Trees Regression algorithm, its working principles, and how to implement it in Python."
      ],
      "metadata": {
        "id": "u5ulfUFxBnqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n",
        "from scipy.stats import f_oneway\n",
        "import plotly.graph_objects as go\n",
        "import graphviz"
      ],
      "metadata": {
        "id": "7TXYd3MRqNy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data and Data Preparation\n",
        "\n",
        "We will use the Boston dataset from the fetch_openml} package. The dataset contains various features related to housing in Boston and their corresponding median house values. The goal is to predict the median house value based on the features."
      ],
      "metadata": {
        "id": "4C3q2_XYrHmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "np.random.seed(123)\n",
        "\n",
        "# Load California Housing data (replacement for deprecated load_boston)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "california = fetch_california_housing()\n",
        "data = pd.DataFrame(data=california.data, columns=california.feature_names)\n",
        "data['medv'] = california.target  # Median house value (in $100,000s)\n",
        "\n",
        "# Ensure all columns are numeric\n",
        "for col in data.columns:\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "# Check for non-numeric columns\n",
        "if data.drop('medv', axis=1).dtypes.apply(lambda x: not np.issubdtype(x, np.number)).any():\n",
        "    raise ValueError(\"Non-numeric features detected in the dataset. Please ensure all features are numeric.\")\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_idx, test_idx = train_test_split(range(len(data)), train_size=0.7, random_state=123)\n",
        "train_data = data.iloc[train_idx]\n",
        "test_data = data.iloc[test_idx]"
      ],
      "metadata": {
        "id": "NRWK-7jzcWJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### Custom CIT splitter for regression (using F-test)\n",
        "\n",
        " This code defines a function `custom_cit_splitter` that finds the best way to split data for a regression tree. It iterates through features and potential split points, using an F-test to find the split that best explains the variation in the continuous target variable (lowest p-value). It returns the best feature, threshold, p-value, and the indices for the resulting split."
      ],
      "metadata": {
        "id": "VSyPT9wVqd1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Custom CIT splitter for regression (using F-test)\n",
        "def custom_cit_splitter(X, y, mincriterion=0.999):\n",
        "    best_pvalue = 1.0\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "    best_split = None\n",
        "\n",
        "    for feature in X.columns:\n",
        "        values = np.sort(X[feature].dropna().unique())\n",
        "        if len(values) < 2:\n",
        "            continue\n",
        "        thresholds = (values[:-1] + values[1:]) / 2\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            left_mask = X[feature] <= threshold\n",
        "            right_mask = X[feature] > threshold\n",
        "            if left_mask.sum() < 5 or right_mask.sum() < 5:\n",
        "                continue\n",
        "\n",
        "            groups = (X[feature] <= threshold).astype(int)\n",
        "            group_values = [y[groups == g] for g in groups.unique()]\n",
        "            if len(group_values) < 2:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                _, pvalue = f_oneway(*group_values)\n",
        "                if np.isnan(pvalue):\n",
        "                    continue\n",
        "                if pvalue < best_pvalue:\n",
        "                    best_pvalue = pvalue\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "                    best_split = {'left_mask': left_mask, 'right_mask': right_mask}\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    return best_feature, best_threshold, best_pvalue, best_split"
      ],
      "metadata": {
        "id": "VrQqJHD6qah2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CIT Regressor class\n",
        "\n",
        "This code defines a `CITRegressor clas`s. It's a wrapper around scikit-learn's `DecisionTreeRegressor` that includes a custom splitting method based on statistical tests (`custom_cit_splitter`) and also performs cost-complexity pruning to simplify the tree. It has methods for fitting the model, making predictions, and getting the number of nodes in the tree."
      ],
      "metadata": {
        "id": "T7BNLEi1qxfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CIT Regressor class\n",
        "class CITRegressor:\n",
        "    def __init__(self, mincriterion=0.999, max_depth=3, min_samples_split=5):\n",
        "        self.mincriterion = mincriterion\n",
        "        self.tree = DecisionTreeRegressor(\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            random_state=123\n",
        "        )\n",
        "        self.custom_splits = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        best_feature, best_threshold, best_pvalue, best_split = custom_cit_splitter(X, y, self.mincriterion)\n",
        "        if best_pvalue > 1 - self.mincriterion:\n",
        "            self.tree.fit(X, y)  # Fallback to simple tree\n",
        "        else:\n",
        "            self.custom_splits.append((best_feature, best_threshold, best_pvalue))\n",
        "            self.tree.fit(X, y)\n",
        "\n",
        "        # Prune tree using cost-complexity pruning\n",
        "        path = self.tree.cost_complexity_pruning_path(X, y)\n",
        "        ccp_alphas = path.ccp_alphas\n",
        "        node_counts = []\n",
        "        for ccp_alpha in ccp_alphas:\n",
        "            tree = DecisionTreeRegressor(\n",
        "                max_depth=self.tree.max_depth,\n",
        "                min_samples_split=self.tree.min_samples_split,\n",
        "                random_state=123,\n",
        "                ccp_alpha=ccp_alpha\n",
        "            )\n",
        "            tree.fit(X, y)\n",
        "            node_counts.append(tree.tree_.node_count)\n",
        "\n",
        "        # Select ccp_alpha to keep tree size reasonable (e.g., <= 10 nodes)\n",
        "        target_nodes = 10\n",
        "        ccp_alpha = ccp_alphas[np.argmin(np.abs(np.array(node_counts) - target_nodes))]\n",
        "        self.tree = DecisionTreeRegressor(\n",
        "            max_depth=self.tree.max_depth,\n",
        "            min_samples_split=self.tree.min_samples_split,\n",
        "            random_state=123,\n",
        "            ccp_alpha=ccp_alpha\n",
        "        )\n",
        "        self.tree.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.tree.predict(X)\n",
        "\n",
        "    def get_n_nodes(self):\n",
        "        return self.tree.tree_.node_count"
      ],
      "metadata": {
        "id": "9hz6mdHkqxpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit Conditional Inference Tree (CIT)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fHm2C5TnBsud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "cit_model = CITRegressor(mincriterion=0.999, max_depth=3, min_samples_split=5)\n",
        "cit_model.fit(train_data.drop('medv', axis=1), train_data['medv'])"
      ],
      "metadata": {
        "id": "FlheLU85BtdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96cb8d3a-9453-48af-b0be-defbee0b02e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.CITRegressor at 0x791c23d11c90>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prune tree (check number of nodes)\n",
        "print(f\"CIT Number of Nodes: {cit_model.get_n_nodes()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOd4ugV6B4_r",
        "outputId": "74b964f0-ec1e-415f-8818-a7de055f1c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIT Number of Nodes: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot Tree"
      ],
      "metadata": {
        "id": "xW_k75k-B8Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot tree with graphviz\n",
        "dot_data = export_graphviz(\n",
        "    cit_model.tree,\n",
        "    out_file=None,\n",
        "    feature_names=train_data.drop('medv', axis=1).columns,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "uJAWf0_rCBRY",
        "outputId": "68c1514e-d876-4692-e79b-82577ae6c760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"666pt\" height=\"373pt\"\n viewBox=\"0.00 0.00 666.00 373.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 369)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-369 662,-369 662,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#f9e0ce\" stroke=\"black\" d=\"M392,-365C392,-365 265,-365 265,-365 259,-365 253,-359 253,-353 253,-353 253,-309 253,-309 253,-303 259,-297 265,-297 265,-297 392,-297 392,-297 398,-297 404,-303 404,-309 404,-309 404,-353 404,-353 404,-359 398,-365 392,-365\"/>\n<text text-anchor=\"start\" x=\"281.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">MedInc ≤ 5.086</text>\n<text text-anchor=\"start\" x=\"261\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.335</text>\n<text text-anchor=\"start\" x=\"276\" y=\"-319.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 14447</text>\n<text text-anchor=\"start\" x=\"287.5\" y=\"-304.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 2.075</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#fcefe5\" stroke=\"black\" d=\"M308,-261C308,-261 181,-261 181,-261 175,-261 169,-255 169,-249 169,-249 169,-205 169,-205 169,-199 175,-193 181,-193 181,-193 308,-193 308,-193 314,-193 320,-199 320,-205 320,-205 320,-249 320,-249 320,-255 314,-261 308,-261\"/>\n<text text-anchor=\"start\" x=\"201\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">MedInc ≤ 3.13</text>\n<text text-anchor=\"start\" x=\"177\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.831</text>\n<text text-anchor=\"start\" x=\"192\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 11478</text>\n<text text-anchor=\"start\" x=\"203.5\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 1.744</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M301.23,-296.88C293.97,-288.07 286.03,-278.43 278.46,-269.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"280.98,-266.79 271.92,-261.3 275.58,-271.24 280.98,-266.79\"/>\n<text text-anchor=\"middle\" x=\"269.51\" y=\"-282.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#eda775\" stroke=\"black\" d=\"M477,-261C477,-261 350,-261 350,-261 344,-261 338,-255 338,-249 338,-249 338,-205 338,-205 338,-199 344,-193 350,-193 350,-193 477,-193 477,-193 483,-193 489,-199 489,-205 489,-205 489,-249 489,-249 489,-255 483,-261 477,-261\"/>\n<text text-anchor=\"start\" x=\"366.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">MedInc ≤ 6.869</text>\n<text text-anchor=\"start\" x=\"346\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.221</text>\n<text text-anchor=\"start\" x=\"365\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2969</text>\n<text text-anchor=\"start\" x=\"372.5\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 3.356</text>\n</g>\n<!-- 0&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>0&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M356.1,-296.88C363.45,-288.07 371.47,-278.43 379.14,-269.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"382.04,-271.22 385.75,-261.3 376.66,-266.74 382.04,-271.22\"/>\n<text text-anchor=\"middle\" x=\"388.01\" y=\"-282.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M139,-149.5C139,-149.5 12,-149.5 12,-149.5 6,-149.5 0,-143.5 0,-137.5 0,-137.5 0,-108.5 0,-108.5 0,-102.5 6,-96.5 12,-96.5 12,-96.5 139,-96.5 139,-96.5 145,-96.5 151,-102.5 151,-108.5 151,-108.5 151,-137.5 151,-137.5 151,-143.5 145,-149.5 139,-149.5\"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.563</text>\n<text text-anchor=\"start\" x=\"27\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5743</text>\n<text text-anchor=\"start\" x=\"34.5\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 1.375</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M189.63,-192.88C169.31,-180.62 146.38,-166.78 126.43,-154.74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"128.15,-151.69 117.78,-149.52 124.54,-157.68 128.15,-151.69\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#f8decc\" stroke=\"black\" d=\"M308,-157C308,-157 181,-157 181,-157 175,-157 169,-151 169,-145 169,-145 169,-101 169,-101 169,-95 175,-89 181,-89 181,-89 308,-89 308,-89 314,-89 320,-95 320,-101 320,-101 320,-145 320,-145 320,-151 314,-157 308,-157\"/>\n<text text-anchor=\"start\" x=\"188.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">AveOccup ≤ 2.412</text>\n<text text-anchor=\"start\" x=\"177\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.827</text>\n<text text-anchor=\"start\" x=\"196\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5735</text>\n<text text-anchor=\"start\" x=\"203.5\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 2.113</text>\n</g>\n<!-- 1&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>1&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M244.5,-192.88C244.5,-184.78 244.5,-175.98 244.5,-167.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"248,-167.3 244.5,-157.3 241,-167.3 248,-167.3\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#f2c29e\" stroke=\"black\" d=\"M139,-53C139,-53 12,-53 12,-53 6,-53 0,-47 0,-41 0,-41 0,-12 0,-12 0,-6 6,0 12,0 12,0 139,0 139,0 145,0 151,-6 151,-12 151,-12 151,-41 151,-41 151,-47 145,-53 139,-53\"/>\n<text text-anchor=\"start\" x=\"8\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.255</text>\n<text text-anchor=\"start\" x=\"27\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1455</text>\n<text text-anchor=\"start\" x=\"34.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 2.764</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M185.39,-88.95C167.48,-78.93 147.89,-67.98 130.27,-58.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"131.69,-54.91 121.26,-53.09 128.28,-61.02 131.69,-54.91\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#fae8db\" stroke=\"black\" d=\"M308,-53C308,-53 181,-53 181,-53 175,-53 169,-47 169,-41 169,-41 169,-12 169,-12 169,-6 175,0 181,0 181,0 308,0 308,0 314,0 320,-6 320,-12 320,-12 320,-41 320,-41 320,-47 314,-53 308,-53\"/>\n<text text-anchor=\"start\" x=\"177\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.488</text>\n<text text-anchor=\"start\" x=\"196\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4280</text>\n<text text-anchor=\"start\" x=\"203.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 1.892</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M244.5,-88.95C244.5,-80.72 244.5,-71.85 244.5,-63.48\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"248,-63.24 244.5,-53.24 241,-63.24 248,-63.24\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#f0b990\" stroke=\"black\" d=\"M477,-157C477,-157 350,-157 350,-157 344,-157 338,-151 338,-145 338,-145 338,-101 338,-101 338,-95 344,-89 350,-89 350,-89 477,-89 477,-89 483,-89 489,-95 489,-101 489,-101 489,-145 489,-145 489,-151 483,-157 477,-157\"/>\n<text text-anchor=\"start\" x=\"357.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">AveOccup ≤ 2.746</text>\n<text text-anchor=\"start\" x=\"346\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.917</text>\n<text text-anchor=\"start\" x=\"365\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2052</text>\n<text text-anchor=\"start\" x=\"372.5\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 2.968</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M413.5,-192.88C413.5,-184.78 413.5,-175.98 413.5,-167.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"417,-167.3 413.5,-157.3 410,-167.3 417,-167.3\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M646,-149.5C646,-149.5 519,-149.5 519,-149.5 513,-149.5 507,-143.5 507,-137.5 507,-137.5 507,-108.5 507,-108.5 507,-102.5 513,-96.5 519,-96.5 519,-96.5 646,-96.5 646,-96.5 652,-96.5 658,-102.5 658,-108.5 658,-108.5 658,-137.5 658,-137.5 658,-143.5 652,-149.5 646,-149.5\"/>\n<text text-anchor=\"start\" x=\"515\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.814</text>\n<text text-anchor=\"start\" x=\"537.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 917</text>\n<text text-anchor=\"start\" x=\"541.5\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 4.224</text>\n</g>\n<!-- 6&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>6&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M468.37,-192.88C488.69,-180.62 511.62,-166.78 531.57,-154.74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"533.46,-157.68 540.22,-149.52 529.85,-151.69 533.46,-157.68\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#eca26e\" stroke=\"black\" d=\"M477,-53C477,-53 350,-53 350,-53 344,-53 338,-47 338,-41 338,-41 338,-12 338,-12 338,-6 344,0 350,0 350,0 477,0 477,0 483,0 489,-6 489,-12 489,-12 489,-41 489,-41 489,-47 483,-53 477,-53\"/>\n<text text-anchor=\"start\" x=\"346\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 1.008</text>\n<text text-anchor=\"start\" x=\"368.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 857</text>\n<text text-anchor=\"start\" x=\"372.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 3.468</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M413.5,-88.95C413.5,-80.72 413.5,-71.85 413.5,-63.48\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"417,-63.24 413.5,-53.24 410,-63.24 417,-63.24\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#f4c8a9\" stroke=\"black\" d=\"M646,-53C646,-53 519,-53 519,-53 513,-53 507,-47 507,-41 507,-41 507,-12 507,-12 507,-6 513,0 519,0 519,0 646,0 646,0 652,0 658,-6 658,-12 658,-12 658,-41 658,-41 658,-47 652,-53 646,-53\"/>\n<text text-anchor=\"start\" x=\"515\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">squared_error = 0.545</text>\n<text text-anchor=\"start\" x=\"534\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1195</text>\n<text text-anchor=\"start\" x=\"545\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 2.61</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M472.61,-88.95C490.52,-78.93 510.11,-67.98 527.73,-58.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"529.72,-61.02 536.74,-53.09 526.31,-54.91 529.72,-61.02\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x791c29dd6990>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction and Evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "lg_ZD97uCOuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate\n",
        "cit_predictions = cit_model.predict(test_data.drop('medv', axis=1))\n",
        "cit_mse = ((cit_predictions - test_data['medv']) ** 2).mean()\n",
        "print(f\"CIT Regression MSE: {cit_mse}\")\n"
      ],
      "metadata": {
        "id": "oJ4JpXtHCROk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b70275-697e-4de8-ea2d-a463c0da5f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIT Regression MSE: 0.6561349415580614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Variable Importance"
      ],
      "metadata": {
        "id": "E3P4ZloRr0B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate variable importance\n",
        "perm_importance = permutation_importance(\n",
        "    cit_model.tree,\n",
        "    train_data.drop('medv', axis=1),\n",
        "    train_data['medv'],\n",
        "    n_repeats=10,\n",
        "    random_state=123\n",
        ")\n",
        "var_imp_cit = pd.DataFrame({\n",
        "    'Variable': train_data.drop('medv', axis=1).columns,\n",
        "    'Importance': perm_importance.importances_mean\n",
        "})\n",
        "var_imp_cit = var_imp_cit.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot variable importance\n",
        "plt.figure(figsize=(5, 4.5))\n",
        "sns.barplot(data=var_imp_cit, x='Importance', y='Variable')\n",
        "plt.title('Variable Importance for CIT-Regression')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Variable')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "hsCu6dO4CdHZ",
        "outputId": "dabd0891-5c5a-408e-f29b-a9aa835181fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x450 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusions\n",
        "\n",
        "This notebook provides an overview and implementation of Conditional Inference Trees (CITs), highlighting their key difference from traditional decision trees: using statistical hypothesis tests for unbiased splitting. The notebook details the CIT algorithm, covering how splits are selected based on p-values and how the tree is built recursively. It then demonstrates how to implement CITs in Python, both with a simplified version built from basic libraries and by adapting scikit-learn's decision tree models to incorporate CIT-like splitting and pruning. Examples are provided for both classification and regression tasks, including steps for data handling, model fitting, making predictions, and evaluating the results. Overall, the notebook serves as a guide to understanding the principles of CITs and applying them in Python.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Nb0kdVIuGlK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "\n",
        "Hastie, T., et al. (2009). The Elements of Statistical Learning. Springer.\n",
        "\n",
        "James, G., et al. (2013). An Introduction to Statistical Learning. Springer.\n",
        "\n",
        "Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\n",
        "\n"
      ],
      "metadata": {
        "id": "vBv6P7rHdyQy"
      }
    }
  ]
}